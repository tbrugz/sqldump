#
# TODO: copy/rename to sqldump.properties
#

###############################################################################
# sqldump config file                                                         #
# https://bitbucket.org/tbrugz/sqldump                                        #
###############################################################################

#
# includes other properties files
#
#@includes = other.properties, yet-another.properties

# just a variable for using ahead - name has no special meaning 
outputdir=output

# properties can use fixed/constant property: ${propfilebasedir} - prop that contains properties file directory

#
# filepattern output props
# patterns that may be used: [schemaname], [objecttype], [objectname]
#
sqldump.mainoutputfilepattern=${outputdir}/[schemaname]_[objecttype].sql
sqldump.outputobjectwithreferencingtable=grant, index
#sqldump.outputfilepattern.maptype.PROCEDURE=EXECUTABLE
#sqldump.outputfilepattern.maptype.TRIGGER=EXECUTABLE
#sqldump.outputfilepattern.maptype.FUNCTION=EXECUTABLE
#sqldump.outputfilepattern.bytype.EXECUTABLE=${outputdir}/EXECUTABLEs.sql

# to directly dump scripts to another database (use instead of 'sqldump.mainoutputfilepattern')
#sqldump.schemadump.output.connpropprefix=sqldump.conn-v2

# other sqldump.outputfilepattern examples:
#sqldump.outputfilepattern=output/[schemaname]/[objecttype]/[objectname].sql
#sqldump.outputfilepattern=output/[objecttype]/[schemaname]_[objectname].sql
#sqldump.outputobjectwithreferencingtable=grant, index, fk, trigger

# other outputfilepattern by object types. "sqldump.outputobjectwithreferencingtable" takes precedence
#sqldump.outputfilepattern.bytype.TABLE=output/TABLE.sql
#sqldump.outputfilepattern.bytype.VIEW=output/VIEW.sql
#sqldump.outputfilepattern.bytype.INDEX=output/INDEX.sql
#sqldump.outputfilepattern.bytype.EXECUTABLE=output/EXECUTABLE.sql
#sqldump.outputfilepattern.bytype.TRIGGER=output/TRIGGER.sql
#sqldump.outputfilepattern.bytype.SEQUENCE=output/SEQUENCE.sql
#sqldump.outputfilepattern.bytype.SYNONYM=output/SYNONYM.sql
#sqldump.outputfilepattern.bytype.GRANT=output/GRANTS.sql
#sqldump.outputfilepattern.bytype.FK=output/FK.sql

#
# delete files
# option option to delete initial output dir contents (except special hidden files (unix dotfiles, eg: .svn, .git, .hg) and files that starts with "_")
#
sqldump.deleteregularfilesfromdir=${outputdir}

# deprecated, use 'sqldump.processingclasses=SQLTests'
#sqldump.dotests=false

#
# sqldump main properties (grab/dump classes)
#

# grabclass can be: JDBCSchemaGrabber, JAXBSchemaXMLSerializer, SchemaSerializer,
#    CastorSchemaXMLSerializer, JSONSchemaSerializer
sqldump.schemagrab.grabclass=JDBCSchemaGrabber

# dumpclasses can be: 
#    SchemaModelScriptDumper, graph.Schema2GraphML,
#    SchemaSerializer, JAXBSchemaXMLSerializer, CastorSchemaXMLSerializer,
#    xtradumpers.AlterSchemaSuggester, mondrianschema.MondrianSchemaDumper,
#    JSONSchemaSerializer
sqldump.schemadump.dumpclasses=SchemaModelScriptDumper, JAXBSchemaXMLSerializer, graph.Schema2GraphML

# processingclasses can be: graph.ResultSet2GraphML, 
#    DataDump, SQLQueries, CascadingDataDump, 
#    SQLDialectTransformer, xtraproc.ModelSQLIdTransformer, SchemaModelTransformer,
#    mondrianschema.Olap4jConnector, mondrianschema.MondrianSchemaValidator, mondrianschema.Olap4jMDXQueries,
#    mondrianschema.MondrianSchema2GraphProcessor

sqldump.processingclasses=graph.ResultSet2GraphML

# if sqldump should fail on error (default is true)
sqldump.failonerror=true|false

#
# schema grab properties (JDBCSchemaGrabber) (defaults below)
#

#sqldump.schemagrab.tables=true
#sqldump.schemagrab.pks=true
#sqldump.schemagrab.fks=true
#sqldump.schemagrab.exportedfks=false
#sqldump.schemagrab.grants=false
## sqldump.dbspecificfeatures.grabindexes may be used instead of sqldump.doschemadump.indexes (really better performance on oracle)
#sqldump.doschemadump.indexes=false
#sqldump.schemagrab.proceduresandfunctions=true
# Table types to grab. Any enum of tbrugz.sqldump.dbmodel.TableType (like TABLE, SYNONYM, SYSTEM_TABLE, VIEW, MATERIALIZED_VIEW, EXTERNAL_TABLE)
# default is null (meaning: all)
#sqldump.schemagrab.tabletypes=TABLE, VIEW
#sqldump.doschemadump.ignoretableswithzerocolumns=true
#sqldump.schemagrab.setconnectionreadonly=false

# schema info: domain tables
# (used by AlterSchemaSuggester, but may be have other uses, like 'sqldump.datadump.tables', 'sqldump.graphmldump.nodestereotypecontains.domaintable')
#sqldump.schemainfo.domaintables=DOMAIN_TABLE_X, DOMAIN_TABLE_Y

# schema names to dump
sqldump.dumpschemapattern=<DW, SA, EMPDEPT>

# filter tables by name
sqldump.schemagrab.tablefilter=TABLE_X, TABLE_Y

# exclude table filters (regex), splitted by "|"
#sqldump.schemagrab.tablename.excludes=EMPT.* | .*_OLD | ZZZ_.*
# exclude object filters
#sqldump.schemagrab.objectname.excludes=BIN\\$.*

# recursive dump (grab) of tables based on FKs, defaults below (default maxlevel is null)
#sqldump.doschemadump.recursivedumpbasedonfks=false
#sqldump.doschemadump.recursivedumpbasedonfks.exportedfks=false
#sqldump.doschemadump.recursivedumpbasedonfks.maxlevel=3
#sqldump.doschemadump.recursivedumpbasedonfks.deep=false

#
# 'sqldump.fromdbid' selects which specificgrabclass to use, column type conversion, ... 
#
#sqldump.fromdbid=oracle
# autodetect database product
sqldump.fromdbid.autodetect=true

#
# to sql-dialect transform (table columns): add 'SQLDialectTransformer' to 'sqldump.processingclasses'
# & select target sql-dialect (prop 'sqldump.todbid') 
# or set 'sqldump.schematransform.toansi' to 'true'
#
#sqldump.todbid=pgsql
#sqldump.schematransform.toansi=true|false

#
# script dump properties
#
#sqldump.dumpwithschemaname=true
#sqldump.dumpsynonymastable=false
#sqldump.dumpviewastable=false
#sqldump.dumpmaterializedviewastable=false
#sqldump.schemadump.dumpdropstatements=false
#sqldump.schemadump.usecreateorreplace=false
#sqldump.doschemadump.fks.atend=true
# defaults below
#sqldump.schemadump.dumpscriptcomments=true
#sqldump.schemadump.dumpremarks=true
#sqldump.schemadump.fks=true
# order to dump indexes (default is null, which is equal to 'indexname')
#sqldump.schemadump.index.orderby=|indexname|tablename

# sql types to use/ignore column precision (overrides defaults from 'src/dbms-specific.properties')
sqldump.sqltypes.useprecision=FLOAT8
sqldump.sqltypes.ignoreprecision=SMALLINT,BIGINT,INTEGER

# quote all SQL identifiers (defalut: false)
sqldump.schemadump.quoteallsqlidentifiers=false

#
# dbms specific features grab (default: false)
#
sqldump.usedbspecificfeatures=true
#sqldump.dbms.specificgrabclass=tbrugz.sqldump.dbmsfeatures.InformationSchemaFeatures

# grab dbspecific by type (defaults below)
#sqldump.dbspecificfeatures.grabindexes=true
#sqldump.dbspecificfeatures.grabexecutables=true
#sqldump.dbspecificfeatures.grabviews=true
#sqldump.dbspecificfeatures.grabtriggers=true
#sqldump.dbspecificfeatures.grabsynonyms=true
#sqldump.dbspecificfeatures.grabsequences=true
# 'extraconstraints' are those besides PK & FK (like unique and check constraints)
#sqldump.dbspecificfeatures.grabextraconstraints=true

# extra dmbs-specific properties (oracle for now)
#sqldump.dbspecificfeatures.sequencestartwithdump=false
# grabbing FKs from unique keys may slower grabbing significantly (but may grab extra info)
#sqldump.dbspecificfeatures.grabfkfromuk=false
#sqldump.dbspecificfeatures.dumpphysicalattributes=true
#sqldump.dbspecificfeatures.dumplogging=true
#sqldump.dbspecificfeatures.dumppartition=true

#
# processor: xtraproc.ModelSQLIdTransformer: transforms sql identifiers
# 
# 2 options: toupper, tolower
#
sqldump.proc.sqlidtransformer=tolower|toupper
#sqldump.proc.sqlidtransformer.iddecorator=...
#sqldump.proc.sqlidtransformer.coltypedecorator=...

#
# data dump props
#
# for datadumping: add DataDump to 'sqldump.processingclasses'

# types of table to dump (default: all but VIEW, MATERIALIZED_VIEW)
#sqldump.datadump.tabletypes=TABLE, SYNONYM, SYSTEM_TABLE, VIEW, MATERIALIZED_VIEW, EXTERNAL_TABLE

# filter tables to dump data
#sqldump.datadump.tables=TABLE_X, TABLE_Y, TABLE_Z
#sqldump.datadump.tables=${sqldump.schemainfo.domaintables}, TABLE_Z

# order table data by PK (dafault is true)
#sqldump.datadump.orderbypk=true

# ignore tables filter ("|" separated regexes)
#sqldump.datadump.ignoretables=TABLE_ABC.* | TABLE_DEF.*

# dump syntaxes (any of: blob, insertinto, csv, json, xml, html, ffc, updatebypk, turtle, iidb)
sqldump.datadump.dumpsyntaxes=insertinto, csv

# default prop: sqldump.datadump.outfilepattern
# patterns that may be used: [tablename], [id] (table/query id), [partitionby], [syntaxfileext]
# sqldump.datadump.outfilepattern=${outputdir}/data/[tablename].[syntaxfileext]
sqldump.datadump.outfilepattern=${outputdir}/data/[tablename][partitionby].[syntaxfileext]

# create empty data files if query returned no data? (default is false)
#sqldump.datadump.createemptyfiles=false|true

# when using partitionby with date column, you may define a dateformat (default below)
#sqldump.datadump.partitionby.dateformat=yyyy-MM-dd

# append to existing files (default is false)
#sqldump.datadump.writeappend=false

# log dump for each X rows, default is 10000 (logger is tbrugz.sqldump.DataDump.datadump-row)
#sqldump.datadump.logeachxrows=100
# log 1st row dumped
#sqldump.datadump.log1strow=true

# charset:: default: UTF-8; options: ISO-8859-1, UTF-16, US-ASCII, ...
#sqldump.datadump.charset=UTF-8
# write (or not) BOM (Byte order mark) - defaults to true for UTF-8, false otherwise
#sqldump.datadump.writebom=true|false|<null>
# default date format: ''yyyy-MM-dd''
sqldump.datadump.dateformat=''yyyy-MM-dd''
# oracle TO_DATE syntax: sqldump.datadump.dateformat='TO_DATE('''yyyy-MM-dd''',''YYYY-MM-DD'')'
sqldump.datadump.rowlimit=100
sqldump.datadump.orderbypk=true

# table-based datadump properties
#sqldump.datadump.TABLE_X.rowlimit=500
#sqldump.datadump.TABLE_X.where=NAME like 'Bill %'
#sqldump.datadump.TABLE_Y.columns=id, name
#sqldump.datadump.TABLE_Y.order=TYPE, NAME

# syntax-specific props: sqldump.datadump.outfilepattern.syntax@<syntaxid>
#sqldump.datadump.outfilepattern.syntax@insertinto=${outputdir}/data_[tablename].sql
#sqldump.datadump.outfilepattern.syntax@csv=${outputdir}/data_[tablename].csv
#sqldump.datadump.outfilepattern.syntax@json=${outputdir}/data_[tablename].json
#sqldump.datadump.outfilepattern.syntax@xml=${outputdir}/data_[tablename].xml


#
# InsertInfo syntax props
#
sqldump.datadump.insertinto.withcolumnnames=true
sqldump.datadump.insertinto.dateformat='TO_DATE('''yyyy-MM-dd''',''YYYY-MM-DD'')'
sqldump.datadump.insertinto.dumpcursors=false

#
# CSV syntax props
#
sqldump.datadump.csv.tablenameheader=false
sqldump.datadump.csv.columnnamesheader=true
sqldump.datadump.csv.recorddelimiter=\r\n
sqldump.datadump.csv.columndelimiter=;
sqldump.datadump.csv.enclosing="
sqldump.datadump.csv.nullvalue=
sqldump.datadump.csv.floatlocale=en
sqldump.datadump.csv.floatformat=###0.000

# JSON syntax props
sqldump.datadump.json.dateformat='"'yyyy-MM-dd'T'HH:mm:ss.SSSZ'"'

# XML syntax props
sqldump.datadump.xml.dumpnullvalues=true|false
sqldump.datadump.xml.nullvalue=nil
sqldump.datadump.xml.rowelement=myrow
sqldump.datadump.xml.dumprowelement=true|false
sqldump.datadump.xml.rowelement4table@<table-name>=table_x_rows
sqldump.datadump.xml.dumprowelement4table@<table-name>=true|false

# HTML syntax props
sqldump.datadump.html.nullvalue=
sqldump.datadump.html.prepend=<!DOCTYPE html>\n<html><head><link rel="stylesheet" type="text/css" href="sqldump.css" /></head><body>\n
sqldump.datadump.html.append=\n</body></html>
#TODO? sqldump.datadump.html.floatlocale

# FFC (formatted fixed column) syntax props
#sqldump.datadump.ffc.linegroupsize=30
#sqldump.datadump.ffc.columndelimiter=\ ;\ 
#sqldump.datadump.ffc.nullvalue=<null>
#sqldump.datadump.ffc.showcolnames=false
#sqldump.datadump.ffc.showcolnameslines=false
sqldump.datadump.ffc.floatlocale=pt

# BLOB syntax props
# blobs may work better if are the first in 'sqldump.datadump.dumpsyntaxes' prop
sqldump.datadump.blob.outfilepattern=${outputdir}/data/[tablename]_[columnname]_[rowid].[syntaxfileext]

# Turtle syntax props
# base URL for 'RDF-based' syntaxes
#sqldump.rdf.base=http://example.org/
# defaults below
#sqldump.datadump.turtle.keycolseparator=;
#sqldump.datadump.turtle.keyincludescolname=true
# appends constant string to keys (default is null)
#sqldump.datadump.turtle.keyappend=#this

# iidb (InsertIntoDatabase) syntax props
#
# '.connpropprefix' defines properties for connecting into another database for data export
#sqldump.datadump.iidb.connpropprefix=iidb.h2connection
# properties for export-database connection
#iidb.h2connection.driverclass=org.h2.Driver
#iidb.h2connection.dburl=jdbc:h2:~/export-db
# extra props (defaults below)
#sqldump.datadump.iidb.autocommit=false
#sqldump.datadump.iidb.batchmode=false
#sqldump.datadump.iidb.commitsize=100
# if '.dropcreatetables' is set to true, iidb attempts to drop & create tables - use with caution
#sqldump.datadump.iidb.dropcreatetables=false
# writes insert to file if error when inserting into database (default is false)
#sqldump.datadump.iidb.fallbacktofile=false


#
# sql queries
#
# for sqlqueries dumping: use 'sqldump.processingclasses=SQLQueries'

#sqldump.queries=table_x, q2, partitiontest, qpivot
# extra options for sqlqueries [defaults at first]
#sqldump.queries.addtomodel=false|true
#sqldump.queries.runqueries=true|false
#sqldump.queries.schemaname=MY_QUERIES_SCHEMA

#sqldump.query.table_x.sql=\
#select * from TABLE_X where abc=?
#sqldump.query.table_x.schemaname=TABLEX_SCHEMA
#sqldump.query.table_x.name=TABLE_X
#sqldump.query.table_x.rowlimit=10000
#sqldump.query.table_x.keycols=ID
#sqldump.query.table_x.param.1=3
# query dump syntaxes takes precedence over default datadump syntaxes (sqldump.datadump.dumpsyntaxes)
#sqldump.query.table_x.dumpsyntaxes=csv, json, xml

#sqldump.query.q2.sqlfile=c:/xxx/query2.sql
#sqldump.query.q2.name=query_2
#sqldump.query.q2.keycols=ID, PARENT_ID
#sqldump.query.q2.cols=NAME:VARCHAR2, DESCRIPTION, TYPE_ID:INTEGER
# replaces '${sqldump.query.replace.<1-n>}' in sql statement by value of 'sqldump.query.<query-id>.replace.<1-n>'
#sqldump.query.q2.replace.1=2011
#sqldump.datadump.outfilepattern.id@q2=${outputdir}/data_q2_[tablename].[syntaxfileext]

#sqldump.query.partitiontest.name=partition_test
#sqldump.query.partitiontest.sql=\
#  select group_id, id, name\
#  from partition_table_test
#sqldump.query.partitiontest.partitionby=_[col:group_id]
# multiple partitionby-patterns may be defined
#sqldump.query.partitiontest.partitionby=| _[col:group_id] | _[col:group_id]_[col:id]
#sqldump.datadump.outfilepattern.id@partitiontest.syntax@csv=${outputdir}/data_partitiontest_csv_[tablename][partitionby].csv
#sqldump.datadump.outfilepattern.id@partitiontest=${outputdir}/data_partitiontest_[tablename][partitionby].[syntaxfileext]

#sqldump.query.qpivot.name=pivot_query
#sqldump.query.qpivot.sql=\
#  select id, category, year, measure\
#  from pivot_table_test
#sqldump.query.qpivot.rsdecoratorfactory=pivot.PivotRSFactory
#sqldump.query.qpivot.rsdecorator.colsnottopivot=id, category
#sqldump.query.qpivot.rsdecorator.colstopivot=year


#
# cascading data dump
#
# for cascading data dumping: use 'sqldump.processingclasses=CascadingDataDump'
#
# determine tables used to start cascading dump
#sqldump.cascadingdd.starttables=TABLE_A, TABLE_B
# tables that will not be dumped
#sqldump.cascadingdd.stoptables=TABLE_X
# do not follow exported keys for these tables - no export! (even if they are start tables)
#sqldump.cascadingdd.noexporttables=TABLE_Z, TABLE_Y
# filters for start tables dump
#sqldump.cascadingdd.filter@TABLE_A=[tablename].year in (2011) and [tablename].id_class in (25, 35, 39)
# props to change cascading dump behavior (defaults below)
#sqldump.cascadingdd.exportedkeys=false
#sqldump.cascadingdd.orderbypk=true


#
# serialization & xml serialization
#
sqldump.serialization.outfile=${outputdir}/schemaModel.ser
sqldump.serialization.infile=${outputdir}/schemaModel.ser
sqldump.serialization.inresource=/schemaModel.ser

sqldump.xmlserialization.jaxb.outfile=${outputdir}/schemaModel.jaxb.xml
sqldump.xmlserialization.jaxb.infile=${outputdir}/schemaModel.jaxb.xml
sqldump.xmlserialization.jaxb.inresource=/schemaModel.jaxb.xml

sqldump.jsonserialization.outfile=${outputdir}/schemaModel.json
sqldump.jsonserialization.infile=${outputdir}/schemaModel.json
sqldump.jsonserialization.inresource=/schemaModel.json

sqldump.xmlserialization.castor.outfile=${outputdir}/schemaModel.castor.xml
sqldump.xmlserialization.castor.infile=${outputdir}/schemaModel.castor.xml


#
# graphML output
#
sqldump.graphmldump.outputfile=${outputdir}/db-schema.graphml
# dump format (default is DumpSchemaGraphMLModel)
#sqldump.graphmldump.dumpformatclass=DumpSchemaGraphMLModel|DumpGXLModel|DumpDotModel

#sqldump.graphmldump.showschemaname=true|false
#sqldump.graphmldump.showconstraints=false|true
#sqldump.graphmldump.showindexes=false|true
#
# default edgelabel: NONE
#sqldump.graphmldump.edgelabel=FK|FKANDCOLUMNS|COLUMNS|NONE
#sqldump.graphmldump.nodeheightbycolsnumber=true|false

# equals-based stereotypes, splitted by "," - stereotype must be defined in a graphml-snippets properties file
#sqldump.graphmldump.nodestereotypecontains.abc=TABLE_X, TABLE_Y
# 'domaintable' is a special stereotype defined in 'graphml-snippets.properties'
#sqldump.graphmldump.nodestereotypecontains.domaintable=${sqldump.schemainfo.domaintables}

# regex-based stereotypes, splitted by "|" - stereotype must be defined in a graphml-snippets properties file
#sqldump.graphmldump.nodestereotyperegex.zzz=ZZZ_.* | .*_ZZZ_.*

#
# include, or not, stereotype 'classes' - defaults below
#sqldump.graphmldump.addtabletypestereotype=true
#sqldump.graphmldump.addschemastereotype=false
# vertex types: root, leaf, connected, disconnected
#sqldump.graphmldump.addvertextypestereotype=false
#
# snippets file (non-default)
#sqldump.graphmldump.snippetsfile=graphml-snippets-simple.properties

#
# graphmlqueries processor
#
sqldump.graphmlqueries=graph1

# mandatory columns for 'edge-sql': SOURCE, TARGET
# optional columns for 'edge-sql': EDGE_TYPE, EDGE_WIDTH
sqldump.graphmlquery.graph1.sql=select SOURCE, TARGET, EDGE_TYPE, EDGE_WIDTH from xxx
#sqldump.graphmlquery.graph1.sqlfile=/home/xxx/edges.sql
# you may use a 'nodesql' query or 2 extra columns in 'edge/default' sql: SOURCE_TYPE, TARGET_TYPE

# mandatory columns for 'node-sql', if present: OBJECT, OBJECT_LABEL
# optional columns for 'node-sql': OBJECT_TYPE, OBJECT_X, OBJECT_Y, OBJECT_WIDTH, OBJECT_HEIGHT, OBJECT_DESC
sqldump.graphmlquery.graph1.nodesql=select OBJECT, OBJECT_TYPE, OBJECT_LABEL, OBJECT_DESC from xxx
#sqldump.graphmlquery.graph1.nodesqlfile=/home/xxx/nodes.sql

sqldump.graphmlquery.graph1.outputfile=${outputdir}/graph1.graphml
# dump format (default is DumpResultSetGraphMLModel)
#sqldump.graphmlquery.graph1.dumpformatclass=DumpResultSetGraphMLModel|DumpGXLModel|DumpDotModel


#sqldump.graphmlquery.graph1.snippetsfile=graphml-snippets-simple.properties

#
# mondrian schema dumper 
# (needs https://bitbucket.org/tbrugz/mondrianschema2graphml - download https://bitbucket.org/tbrugz/mondrianschema2graphml/downloads/mondrianschema2graphml.jar)
#
sqldump.mondrianschema.outfile=${outputdir}/mondrian-schema.xml
# mondrian schema name
sqldump.mondrianschema.schemaname=<schema-name>
# add dimension for each hierarchy
sqldump.mondrianschema.adddimforeachhierarchy=true
# add sql identifier string decorator
sqldump.mondrianschema.sqliddecorator=toupper
# set 'hasAll' property (for all dimensions, default is true)
#sqldump.mondrianschema.hierarchyhasall=false
# determine fact tables (otherwise fact tables are inferred)
sqldump.mondrianschema.facttables=table_fact1, table_fact2
# add fact tables (besides inferred fact tables)
sqldump.mondrianschema.xtrafacttables=table_fact_x
# default measure aggregators (real default is only 'sum')
sqldump.mondrianschema.defaultaggregators=sum, count
# inores dimension tables
sqldump.mondrianschema.ignoredims=dim_table_xx, dim_table_yy
# adds fact-count measure for every cube
sqldump.mondrianschema.factcountmeasure=factcount

# define level name column
sqldump.mondrianschema.level@<dim_table>.levelnamecol=<level_name_column>
# define parent level for dimension column (for classic star schema: multiple levels in one table)
sqldump.mondrianschema.level@<dim_table>.parentLevels=<level_column_1>:<level_name_column_1>, <level_column_2>:<level_name_column_2>
# determine measure columns (otherwise they are inferred)
sqldump.mondrianschema.cube@<fact_table>.measurecols=amount_x, measure_y
# determine measure columns based on regex
sqldump.mondrianschema.cube@<fact_table>.measurecolsregex=amount_.*, measure_.y
# degenerated dimension columns
sqldump.mondrianschema.cube@<fact_table>.degeneratedims=<column_1>, <column_2>:<optional_level_name_column_2>
# override default aggregators for a cube
sqldump.mondrianschema.cube@<fact_table>.aggregators=sum, count
# adds extra measures
sqldump.mondrianschema.cube@<fact_table>.addmeasures=<column>:<aggregator>[:<label>];<another-measure>
# extra dimension tables/columns (when there is no FK)
sqldump.mondrianschema.table@<table>.xtrafk=<fk_column_1>:<schema_dim_table_1>.<dim_table_1>:<dim_column_1>, (...)
# level's internal type (Legal values: {int, long, Object, String})
sqldump.mondrianschema.table@<table>.column@<column>.internalType=long

# ignore cube with no measure (default: true)
#sqldump.mondrianschema.ignorecubewithnomeasure=true
# ignore cube with no dimension (default: true)
#sqldump.mondrianschema.ignorecubewithnodimension=true
# preferred level name columns (are used if dimension table has column that matches it)
sqldump.mondrianschema.preferredlevelnamecolumns=label, name
# add all degenerated dimension candidate columns to schema (dafault: false)
#sqldump.mondrianschema.addalldegeneratedimcandidates=true


#
# mondrian schema validator
#
# jars that are needed in classpath for mondrian validation:
# olap4j.jar, mondrian.jar, 
# commons-collection.jar, commons-dbcp.jar, commons-pool.jar, commons-vfs.jar, commons-math.jar,
# eigenbase-properties.jar, eigenbase-resgen.jar & eigenbase-xom.jar

# schema file to be validated - if not defined, prop 'sqldump.mondrianschema.outfile' is used
sqldump.mondrianvalidator.schemafile=${outputdir}/mondrian-schema.xml
# validator may be used with mondrian schema dumper - prop below must be true (default is false)
sqldump.mondrianschema.validateschema=true
# may be used as 'standalone' - add 'mondrianschema.MondrianSchemaValidator' to processingclasses
#sqldump.processingclasses=mondrianschema.MondrianSchemaValidator


#
# olap4j mdx queries datadump - 'mondrianschema.Olap4jMDXQueries'
#
# depends on previous 'mondrianschema.Olap4jConnector' processor
# MDX queries ids
#sqldump.mdxqueries.ids=q1
# output file pattern ; pattern [tablename] may be used 
#sqldump.mdxqueries.outfilepattern=${outputdir}/[tablename].txt
# RectangularCellSetFormatter's compact mode (false/true) 
#sqldump.mdxqueries.x-compactmode=true
# MDX queries
#sqldump.mdxqueries.q1.query=SELECT NON EMPTY {Hierarchize({[Measures].[factcount]})} ON COLUMNS, NON EMPTY {Hierarchize({{[DIM_OS].[OS_FAMILY].Members}, {[DIM_OS].[DIM_OS].Members}})} ON ROWS FROM [HTTP_LOGS]
#sqldump.mdxqueries.q1.name=query1


#
# MondrianSchema2GraphProcessor
#
#sqldump.mondrianschema2graph.infile=mondrian/demo/FoodMart.xml
# if '.infile' is not defined, 'sqldump.mondrianschema.outfile' is used
# e.g: sqldump.mondrianschema.outfile=${outputdir}/foormartschema-new.xml
#sqldump.mondrianschema2graph.outfile=${outputdir}/foodmart.graphml


#
# schema model transform: add 'SchemaModelTransformer' to 'sqldump.processingclasses'
#
# removes schema name from all objects
#sqldump.modeltransform.removeschemaname=false
# removes tables with referencing FKs
#sqldump.modeltransform.removetableswithfks=TABLE_A, TABLE_B
# removes FKs by name
#sqldump.modeltransform.removefksbyname=FK1, EMP_DEPT_FK, ...
# adds xtra FKs to table <fktable>
#sqldump.modeltransform.table@<fktable>.xtrafk=<fkcolumn-1>[,<fkcolumn-2>...]:[<schema>.]<pktable>:<pkcolumn-1>[,<pkcolumn-2>...][:<fk-name>] [;(...)]
# transform column type 
#sqldump.modeltransform.columntype@<from>=<to>
#sqldump.modeltransform.columntype@DATE=TIMESTAMP # ex: from DATE to TIMESTAMP


#
# alter schema suggestions output
#

# outfilepattern may use [schemaname] and [objecttype]
# sqldump.alterschemasuggester.outfilepattern=${outputdir}/alter-schema-suggestions-[schemaname]-[objecttype].sql
sqldump.alterschemasuggester.outfilepattern=${outputdir}/alter-schema-suggestions.sql
sqldump.alterschemasuggester.simplefksonly=true
#sqldump.alterschemasuggester.alterobjectsfromschemas=SCHEMA_X, SCHEMA_Y


############
#
# connection/driver info 
#

# to select a different connection properties prefix
#sqldump.connpropprefix=sqldump.conn1
# example:
#environment=production
#sqldump.connpropprefix=sqldump.${environment}
#sqldump.production.dburl=jdbc:h2:~/production

#
# reads username/password from console or gui
#
#sqldump.askforusername=true
#sqldump.askforpassword=true
#sqldump.askforusernamegui=true
#sqldump.askforpasswordgui=true

# executes query at connection initialization
#sqldump.initsql=set session LC_NUMERIC to 'English'

# MS Access
#sqldump.driverclass=sun.jdbc.odbc.JdbcOdbcDriver
#sqldump.dburl=jdbc:odbc:Driver={MicroSoft Access Driver (*.mdb)};DBQ=Northwind.mdb
# accdb (not fully supported):
#sqldump.dburl=jdbc:odbc:Driver={Microsoft Access Driver (*.mdb, *.accdb)};DBQ=Northwind.accdb

# Oracle
#sqldump.driverclass=oracle.jdbc.OracleDriver
#sqldump.dburl=jdbc:oracle:thin:@<url>:<port>:<sid>
#sqldump.user=scott
#sqldump.password=tiger

# PostgreSQL
#sqldump.driverclass=org.postgresql.Driver
#sqldump.dburl=jdbc:postgresql://<host>/<db>
#sqldump.user=
#sqldump.password=

# Derby Embedded
#sqldump.driverclass=org.apache.derby.jdbc.EmbeddedDriver
#sqldump.dburl=jdbc:derby:<db-path>
#sqldump.user=sa
#sqldump.password=sa

# HSQLDB In-Process
#sqldump.driverclass=org.hsqldb.jdbc.JDBCDriver
#sqldump.dburl=jdbc:hsqldb:file:<db-path>
#sqldump.user=sa
#sqldump.password=sa

# MySQL
#sqldump.driverclass=com.mysql.jdbc.Driver
#sqldump.dburl=jdbc:mysql://<host>:<port>/<database>
#sqldump.user=
#sqldump.password=

# MariaDB
# info: https://kb.askmonty.org/en/about-the-mariadb-java-client/
#sqldump.driverclass=org.mariadb.jdbc.Driver
#sqldump.dburl=jdbc:mysql://<host>:<port>/<database>
#sqldump.user=
#sqldump.password=

# SQLite
#sqldump.driverclass=org.sqlite.JDBC
#sqldump.dburl=jdbc:sqlite:</path/to/sqlite.db>

# H2
# info: http://www.h2database.com/html/download.html
#sqldump.driverclass=org.h2.Driver
#sqldump.dburl=jdbc:h2:~/test

# Monet DB
# info: http://dev.monetdb.org/downloads/Java/Latest/
#sqldump.driverclass=nl.cwi.monetdb.jdbc.MonetDriver
#sqldump.dburl=jdbc:monetdb://localhost/demo
#sqldump.user=monetdb
#sqldump.password=monetdb

# Drizzle
# info: http://www.drizzle.org/
#sqldump.driverclass=org.drizzle.jdbc.DrizzleDriver
#sqldump.dburl=jdbc:drizzle://<user>@<host>:<port>/<database>
#sqldump.dburl=jdbc:mysql:thin://<user>@<host>:<port>/<database>
#sqldump.user=root
#sqldump.password=

# Firebird
# info: http://jaybirdwiki.firebirdsql.org/jaybird/doku.php?id=config:driver_config
#sqldump.driverclass=org.firebirdsql.jdbc.FBDriver
#sqldump.dburl=jdbc:firebirdsql://host[:port]/<database>
#sqldump.dburl=jdbc:firebirdsql://localhost:3050/examples/empbuild/employee.fdb
#sqldump.user=sysdba
#sqldump.password=masterke

# Virtuoso
# info: http://docs.openlinksw.com/virtuoso/VirtuosoDriverJDBC.html
#sqldump.driverclass=virtuoso.jdbc4.Driver
#sqldump.dburl=jdbc:virtuoso://hostname:port/UID=dba/PWD=dba
#sqldump.dburl=jdbc:virtuoso://localhost
#sqldump.user=dba
#sqldump.password=dba



# Connection from datasource/jndi/pool
#sqldump.datasource=jdbc/LocalTestDB
