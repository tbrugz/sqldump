#
# TODO: copy/rename to sqldump.properties
#

###############################################################################
# sqldump config file                                                         #
# https://bitbucket.org/tbrugz/sqldump                                        #
###############################################################################

#
# includes other properties files
#
#@includes = other.properties, yet-another.properties

# just a variable for using ahead - name has no special meaning 
outputdir=output

# properties can use fixed/constant property: ${propfilebasedir} - prop that contains prop file directory

#
# filepattern output props
#
sqldump.mainoutputfilepattern=${outputdir}/${schemaname}_${objecttype}.sql
sqldump.outputobjectwithreferencingtable=grant, index
#sqldump.outputfilepattern.maptype.PROCEDURE=EXECUTABLE
#sqldump.outputfilepattern.maptype.TRIGGER=EXECUTABLE
#sqldump.outputfilepattern.maptype.FUNCTION=EXECUTABLE
#sqldump.outputfilepattern.bytype.EXECUTABLE=${outputdir}/EXECUTABLEs.sql

# other sqldump.outputfilepattern examples:
#sqldump.outputfilepattern=output/${schemaname}/${objecttype}/${objectname}.sql
#sqldump.outputfilepattern=output/${objecttype}/${schemaname}_${objectname}.sql
#sqldump.outputobjectwithreferencingtable=grant, index, fk, trigger

# other outputfilepattern by object types. "sqldump.outputobjectwithreferencingtable" takes precedence
#sqldump.outputfilepattern.bytype.TABLE=output/TABLE.sql
#sqldump.outputfilepattern.bytype.VIEW=output/VIEW.sql
#sqldump.outputfilepattern.bytype.INDEX=output/INDEX.sql
#sqldump.outputfilepattern.bytype.EXECUTABLE=output/EXECUTABLE.sql
#sqldump.outputfilepattern.bytype.TRIGGER=output/TRIGGER.sql
#sqldump.outputfilepattern.bytype.SEQUENCE=output/SEQUENCE.sql
#sqldump.outputfilepattern.bytype.SYNONYM=output/SYNONYM.sql
#sqldump.outputfilepattern.bytype.GRANT=output/GRANTS.sql
#sqldump.outputfilepattern.bytype.FK=output/FK.sql

#
# delete files
# option option to delete initial output dir contents (except special hidden files (unix dotfiles, eg: .svn, .git, .hg) and files that starts with "_")
#
sqldump.deleteregularfilesfromdir=${outputdir}

# deprecated, use 'sqldump.processingclasses=SQLTests'
#sqldump.dotests=false

#
# sqldump main properties (grab/dump classes)
#

# grabclass can be: JDBCSchemaGrabber, JAXBSchemaXMLSerializer, SchemaSerializer, CastorSchemaXMLSerializer
sqldump.schemagrab.grabclass=JDBCSchemaGrabber

# dumpclasses can be: 
#    SchemaModelScriptDumper, graph.Schema2GraphML, 
#    SchemaSerializer, JAXBSchemaXMLSerializer, CastorSchemaXMLSerializer,  
#    xtradumpers.AlterSchemaSuggester, mondrianschema.MondrianSchemaDumper
sqldump.schemadump.dumpclasses=SchemaModelScriptDumper, JAXBSchemaXMLSerializer, graph.Schema2GraphML

# processingclasses can be: graph.ResultSet2GraphML, 
#    DataDump, SQLQueries, SQLTests, 
#    SQLDialectTransformer, xtraproc.ModelSQLIdTransformer
sqldump.processingclasses=graph.ResultSet2GraphML

#
# schema grab properties (JDBCSchemaGrabber) (defaults below)
#

#sqldump.schemagrab.tables=true
#sqldump.doschemadump.pks=true
#sqldump.doschemadump.fks=true
#sqldump.doschemadump.exportedfks=false
#sqldump.doschemadump.grants=false
## sqldump.dbspecificfeatures.grabindexes may be used instead of sqldump.doschemadump.indexes (really better performance on oracle)
#sqldump.doschemadump.indexes=false
#sqldump.schemagrab.proceduresandfunctions=true
# Table types to grab. Any enum of tbrugz.sqldump.dbmodel.TableType (like TABLE, SYNONYM, SYSTEM_TABLE, VIEW, MATERIALIZED_VIEW, EXTERNAL_TABLE)
# default is null (meaning: all)
#sqldump.schemagrab.tabletypes=TABLE, VIEW
#sqldump.doschemadump.ignoretableswithzerocolumns=true
#sqldump.schemagrab.setconnectionreadonly=false

# schema info: domain tables
# (used by AlterSchemaSuggester, but may be have other uses, like 'sqldump.datadump.tables', 'sqldump.graphmldump.nodestereotypecontains.domaintable')
#sqldump.schemainfo.domaintables=DOMAIN_TABLE_X, DOMAIN_TABLE_Y

# schema names to dump
sqldump.dumpschemapattern=<DW, SA, EMPDEPT>

# filter tables by name
sqldump.schemagrab.tablefilter=TABLE_X, TABLE_Y

# exclude table filters (regex), splitted by "|"
#sqldump.schemadump.tablename.excludes=EMPT.* | .*_OLD | ZZZ_.*
# exclude object filters
#sqldump.schemagrab.objectname.excludes=BIN\\$.*

# recursive dump (grab) of tables based on FKs, defaults below (default maxlevel is null)
#sqldump.doschemadump.recursivedumpbasedonfks=false
#sqldump.doschemadump.recursivedumpbasedonfks.exportedfks=false
#sqldump.doschemadump.recursivedumpbasedonfks.maxlevel=3
#sqldump.doschemadump.recursivedumpbasedonfks.deep=false

#
# 'sqldump.fromdbid' selects which specificgrabclass to use, column type conversion, ... 
#
#sqldump.fromdbid=oracle
# autodetect database product
sqldump.fromdbid.autodetect=true

#
# to sql-dialect transform (table columns): add 'SQLDialectTransformer' to 'sqldump.processingclasses'
# & select target sql-dialect (prop 'sqldump.todbid')
#sqldump.todbid=pgsql

#
# script dump properties
#
#sqldump.dumpwithschemaname=true
#sqldump.dumpsynonymastable=false
#sqldump.dumpviewastable=false
#sqldump.dumpmaterializedviewastable=false
#sqldump.schemadump.dumpdropstatements=false
#sqldump.schemadump.usecreateorreplace=false
#sqldump.doschemadump.fks.atend=true
# defaults below
#sqldump.schemadump.dumpscriptcomments=true
#sqldump.schemadump.dumpremarks=true
# order to dump indexes (default is null, which is equal to 'indexname')
#sqldump.schemadump.index.orderby=|indexname|tablename

# sql types to use/ignore column precision (overrides defaults from 'src/dbms-specific.properties')
sqldump.sqltypes.useprecision=FLOAT8
sqldump.sqltypes.ignoreprecision=SMALLINT,BIGINT,INTEGER

# quote all SQL identifiers (defalut: false)
sqldump.schemadump.quoteallsqlidentifiers=false

#
# dbms specific features grab (default: false)
#
sqldump.usedbspecificfeatures=true
#sqldump.dbms.specificgrabclass=tbrugz.sqldump.dbmsfeatures.InformationSchemaFeatures

# grab dbspecific by type (defaults below)
#sqldump.dbspecificfeatures.grabindexes=true
#sqldump.dbspecificfeatures.grabexecutables=true
#sqldump.dbspecificfeatures.grabviews=true
#sqldump.dbspecificfeatures.grabtriggers=true
#sqldump.dbspecificfeatures.grabsynonyms=true
#sqldump.dbspecificfeatures.grabsequences=true
# 'extraconstraints' are those besides PK & FK (like unique and check constraints)
#sqldump.dbspecificfeatures.grabextraconstraints=true

# extra dmbs-specific properties (oracle for now)
#sqldump.dbspecificfeatures.sequencestartwithdump=false
# grabbing FKs from unique keys may slower grabbing significantly (but may grab extra info)
#sqldump.dbspecificfeatures.grabfkfromuk=false
#sqldump.dbspecificfeatures.dumpphysicalattributes=true
#sqldump.dbspecificfeatures.dumplogging=true

#
# processor: xtraproc.ModelSQLIdTransformer: transforms sql identifiers
# 
# 2 options: toupper, tolower
#
sqldump.proc.sqlidtransformer=tolower|toupper
#sqldump.proc.sqlidtransformer.iddecorator=...
#sqldump.proc.sqlidtransformer.coltypedecorator=...

#
# data dump props
#
# for datadumping: add DataDump to 'sqldump.processingclasses'

# types of table to dump (default: all but VIEW, MATERIALIZED_VIEW)
#sqldump.datadump.tabletypes=TABLE, SYNONYM, SYSTEM_TABLE, VIEW, MATERIALIZED_VIEW, EXTERNAL_TABLE

# filter tables to dump data
#sqldump.datadump.tables=TABLE_X, TABLE_Y, TABLE_Z
#sqldump.datadump.tables=${sqldump.schemainfo.domaintables}, TABLE_Z

# order table data by PK (dafault is true)
#sqldump.datadump.orderbypk=true

# ignore tables filter ("|" separated regexes)
#sqldump.datadump.ignoretables=TABLE_ABC.* | TABLE_DEF.*

# dump syntaxes (any of: blob, insertinto, csv, json, xml, html, ffc, updatebypk, turtle)
sqldump.datadump.dumpsyntaxes=insertinto, csv

# default prop: sqldump.datadump.outfilepattern
# sqldump.datadump.outfilepattern=${outputdir}/data/${tablename}.${syntaxfileext}
sqldump.datadump.outfilepattern=${outputdir}/data/${tablename}${partitionby}.${syntaxfileext}

# create empty data files if query returned no data? (default is false)
#sqldump.datadump.createemptyfiles=false|true

# log dump for each X rows, default is 10000 (logger is tbrugz.sqldump.DataDump.datadump-row)
#sqldump.datadump.logeachxrows=100
# log 1st row dumped
#sqldump.datadump.log1strow=true

# syntax-specific props: sqldump.datadump.outfilepattern.syntax@<syntaxid>
#sqldump.datadump.outfilepattern.syntax@insertinto=${outputdir}/data_${tablename}.sql
#sqldump.datadump.outfilepattern.syntax@csv=${outputdir}/data_${tablename}.csv
#sqldump.datadump.outfilepattern.syntax@json=${outputdir}/data_${tablename}.json
#sqldump.datadump.outfilepattern.syntax@xml=${outputdir}/data_${tablename}.xml

#
# InsertInfo syntax props
#
sqldump.datadump.insertinto.withcolumnnames=true
sqldump.datadump.insertinto.dateformat='TO_DATE('''yyyy-MM-dd''',''YYYY-MM-DD'')'

#
# CSV syntax props
#
sqldump.datadump.csv.tablenameheader=false
sqldump.datadump.csv.columnnamesheader=true
sqldump.datadump.csv.recorddelimiter=\r\n
sqldump.datadump.csv.columndelimiter=;
sqldump.datadump.csv.enclosing="
sqldump.datadump.csv.nullvalue=
sqldump.datadump.csv.floatlocale=en
sqldump.datadump.csv.floatformat=###0.000

# JSON syntax props
sqldump.datadump.json.dateformat='"'yyyy-MM-dd'T'HH:mm:ss.SSSZ'"'

# XML syntax props
sqldump.datadump.xml.nullvalue=nil
sqldump.datadump.xml.rowelement=myrow

# HTML syntax props
sqldump.datadump.html.nullvalue=
sqldump.datadump.html.prepend=<!DOCTYPE html>\n<html><head><link rel="stylesheet" type="text/css" href="sqldump.css" /></head><body>\n
sqldump.datadump.html.append=\n</body></html>
#TODO? sqldump.datadump.html.floatlocale

# FFC (formatted fixed column) syntax props
#sqldump.datadump.ffc.linegroupsize=30
#sqldump.datadump.ffc.columndelimiter=\ ;\ 
#sqldump.datadump.ffc.nullvalue=<null>
#sqldump.datadump.ffc.showcolnames=false
#sqldump.datadump.ffc.showcolnameslines=false
sqldump.datadump.ffc.floatlocale=pt

# BLOB syntax props
# blobs may work better if are the first in 'sqldump.datadump.dumpsyntaxes' prop
sqldump.datadump.blob.outfilepattern=${outputdir}/data/${tablename}_${columnname}_${rowid}.${syntaxfileext}

# Turtle syntax props
# base URL for 'RDF-based' syntaxes
#sqldump.rdf.base=http://example.org/
# defaults below
#sqldump.datadump.turtle.keycolseparator=;
#sqldump.datadump.turtle.keyincludescolname=true
# appends constant string to keys (default is null)
#sqldump.datadump.turtle.keyappend=#this


# charset:: default: UTF-8; options: ISO-8859-1, UTF-16, US-ASCII, ...
#sqldump.datadump.charset=UTF-8
# write (or not) BOM (Byte order mark) - defaults to true for UTF-8, false otherwise
#sqldump.datadump.writebom=true|false|<null>
# default date format: ''yyyy-MM-dd''
sqldump.datadump.dateformat=''yyyy-MM-dd''
# oracle TO_DATE syntax: sqldump.datadump.dateformat='TO_DATE('''yyyy-MM-dd''',''YYYY-MM-DD'')'
sqldump.datadump.rowlimit=100
sqldump.datadump.orderbypk=true

#sqldump.datadump.TABLE_X.rowlimit=500
#sqldump.datadump.TABLE_X.where=NAME like 'Bill %'
#sqldump.datadump.TABLE_Y.columns=id, name
#sqldump.datadump.TABLE_Y.order=TYPE, NAME

#
# sql queries
#
# for sqlqueries dumping: use 'sqldump.processingclasses=SQLQueries'

#sqldump.queries=table_x, q2, partitiontest, qpivot
# extra options for sqlqueries [defaults at first]
#sqldump.queries.addtomodel=false|true
#sqldump.queries.runqueries=true|false
#sqldump.queries.schemaname=MY_QUERIES_SCHEMA

#sqldump.query.table_x.sql=\
#select * from TABLE_X where abc=?
#sqldump.query.table_x.schemaname=TABLEX_SCHEMA
#sqldump.query.table_x.name=TABLE_X
#sqldump.query.table_x.rowlimit=10000
#sqldump.query.table_x.keycols=ID
#sqldump.query.table_x.param.1=3
# query dump syntaxes takes precedence over default datadump syntaxes (sqldump.datadump.dumpsyntaxes)
#sqldump.query.table_x.dumpsyntaxes=csv, json, xml

#sqldump.query.q2.sqlfile=c:/xxx/query2.sql
#sqldump.query.q2.name=query_2
#sqldump.query.q2.keycols=ID, PARENT_ID
#sqldump.query.q2.cols=NAME:VARCHAR2, DESCRIPTION, TYPE_ID:INTEGER
# replaces '${sqldump.query.replace.<1-n>}' in sql statement by value of 'sqldump.query.<query-id>.replace.<1-n>'
#sqldump.query.q2.replace.1=2011
#sqldump.datadump.outfilepattern.id@q2=${outputdir}/data_q2_${tablename}.${syntaxfileext}

#sqldump.query.partitiontest.name=partition_test
#sqldump.query.partitiontest.sql=\
#  select group_id, id, name\
#  from partition_table_test
#sqldump.query.partitiontest.partitionby=_${col:group_id}
# multiple partitionby-patterns may be defined
#sqldump.query.partitiontest.partitionby=| _${col:group_id} | _${col:group_id}_${col:id} 
#sqldump.datadump.outfilepattern.id@partitiontest.syntax@csv=${outputdir}/data_partitiontest_csv_${tablename}${partitionby}.csv
#sqldump.datadump.outfilepattern.id@partitiontest=${outputdir}/data_partitiontest_${tablename}${partitionby}.${syntaxfileext}

#sqldump.query.qpivot.name=pivot_query
#sqldump.query.qpivot.sql=\
#  select id, category, year, measure\
#  from pivot_table_test
#sqldump.query.qpivot.rsdecoratorfactory=pivot.PivotRSFactory
#sqldump.query.qpivot.rsdecorator.colsnottopivot=id, category
#sqldump.query.qpivot.rsdecorator.colstopivot=year


#
# serialization & xml serialization
#
sqldump.serialization.outfile=${outputdir}/schemaModel.ser
sqldump.serialization.infile=${outputdir}/schemaModel.ser
sqldump.serialization.inresource=/schemaModel.ser

sqldump.xmlserialization.jaxb.outfile=${outputdir}/schemaModel.jaxb.xml
sqldump.xmlserialization.jaxb.infile=${outputdir}/schemaModel.jaxb.xml
sqldump.xmlserialization.jaxb.inresource=/schemaModel.jaxb.xml

sqldump.xmlserialization.castor.outfile=${outputdir}/schemaModel.castor.xml
sqldump.xmlserialization.castor.infile=${outputdir}/schemaModel.castor.xml


#
# graphML output
#
sqldump.graphmldump.outputfile=${outputdir}/db-schema.graphml
# dump format (default is DumpSchemaGraphMLModel)
#sqldump.graphmldump.dumpformatclass=DumpSchemaGraphMLModel|DumpGXLModel|DumpDotModel

#sqldump.graphmldump.showschemaname=true|false
#sqldump.graphmldump.showconstraints=false|true
#sqldump.graphmldump.showindexes=false|true
#
# default edgelabel: NONE
#sqldump.graphmldump.edgelabel=FK|FKANDCOLUMNS|COLUMNS|NONE
#sqldump.graphmldump.nodeheightbycolsnumber=true|false

# equals-based stereotypes, splitted by "," - stereotype must be defined in a graphml-snippets properties file
#sqldump.graphmldump.nodestereotypecontains.abc=TABLE_X, TABLE_Y
# 'domaintable' is a special stereotype defined in 'graphml-snippets.properties'
#sqldump.graphmldump.nodestereotypecontains.domaintable=${sqldump.schemainfo.domaintables}

# regex-based stereotypes, splitted by "|" - stereotype must be defined in a graphml-snippets properties file
#sqldump.graphmldump.nodestereotyperegex.zzz=ZZZ_.* | .*_ZZZ_.*

#
# include, or not, stereotype 'classes' - defaults below
#sqldump.graphmldump.addtabletypestereotype=true
#sqldump.graphmldump.addschemastereotype=false
# vertex types: root, leaf, connected, disconnected
#sqldump.graphmldump.addvertextypestereotype=false
#
# snippets file (non-default)
#sqldump.graphmldump.snippetsfile=graphml-snippets-simple.properties

#
# graphmlqueries processor
#
sqldump.graphmlqueries=graph1

# mandatory columns for 'edge-sql': SOURCE, TARGET
# optional columns for 'edge-sql': EDGE_TYPE, EDGE_WIDTH
sqldump.graphmlquery.graph1.sql=select SOURCE, TARGET, EDGE_TYPE, EDGE_WIDTH from xxx
#sqldump.graphmlquery.graph1.sqlfile=/home/xxx/edges.sql
# you may use a 'nodesql' query or 2 extra columns in 'edge/default' sql: SOURCE_TYPE, TARGET_TYPE

# mandatory columns for 'node-sql', if present: OBJECT, OBJECT_LABEL
# optional columns for 'node-sql': OBJECT_TYPE, OBJECT_X, OBJECT_Y, OBJECT_WIDTH, OBJECT_HEIGHT, OBJECT_DESC
sqldump.graphmlquery.graph1.nodesql=select OBJECT, OBJECT_TYPE, OBJECT_LABEL, OBJECT_DESC from xxx
#sqldump.graphmlquery.graph1.nodesqlfile=/home/xxx/nodes.sql

sqldump.graphmlquery.graph1.outputfile=${outputdir}/graph1.graphml
# dump format (default is DumpResultSetGraphMLModel)
#sqldump.graphmlquery.graph1.dumpformatclass=DumpResultSetGraphMLModel|DumpGXLModel|DumpDotModel


#sqldump.graphmlquery.graph1.snippetsfile=graphml-snippets-simple.properties

#
# mondrian schema dumper 
# (needs https://bitbucket.org/tbrugz/mondrianschema2graphml - download https://bitbucket.org/tbrugz/mondrianschema2graphml/downloads/mondrianschema2graphml.jar)
#
sqldump.mondrianschema.outfile=${outputdir}/mondrian-schema.xml
# mondrian schema name
sqldump.mondrianschema.schemaname=<schema-name>
# add dimension for each hierarchy
sqldump.mondrianschema.adddimforeachhierarchy=true
# add sql identifier string decorator
sqldump.mondrianschema.sqliddecorator=toupper
# set 'hasAll' property (for all dimensions, default is true)
#sqldump.mondrianschema.hierarchyhasall=false
# determine fact tables (otherwise fact tables are inferred)
sqldump.mondrianschema.facttables=table_fact1, table_fact2
# add fact tables (besides inferred fact tables)
sqldump.mondrianschema.xtrafacttables=table_fact_x
# default measure aggregators (real default is only 'sum')
sqldump.mondrianschema.defaultaggregators=sum, count
# inores dimension tables
sqldump.mondrianschema.ignoredims=dim_table_xx, dim_table_yy
# adds fact-count measure for every cube
sqldump.mondrianschema.factcountmeasure=factcount

# define level name column
sqldump.mondrianschema.level@<dim_table>.levelnamecol=<level_name_column>
# define parent level for dimension column (for classic star schema: multiple levels in one table)
sqldump.mondrianschema.level@<dim_table>.parentLevels=<level_column_1>:<level_name_column_1>, <level_column_2>:<level_name_column_2>
# determine measure columns (otherwise they are inferred)
sqldump.mondrianschema.cube@<fact_table>.measurecols=amount_x, measure_y
# determine measure columns based on regex
sqldump.mondrianschema.cube@<fact_table>.measurecolsregex=amount_.*, measure_.y
# degenerated dimension columns
sqldump.mondrianschema.cube@<fact_table>.degeneratedims=<column_1>, <column_2>:<optional_level_name_column_2>
# override default aggregators for a cube
sqldump.mondrianschema.cube@<fact_table>.aggregators=sum, count
# adds extra measures
sqldump.mondrianschema.cube@<fact_table>.addmeasures=<column>:<aggregator>[:<label>];<another-measure>
# extra dimension tables/columns (when there is no FK)
sqldump.mondrianschema.table@<table>.xtrafk=<fk_column_1>:<schema_dim_table_1>.<dim_table_1>:<dim_column_1>, (...)

# ignore cube with no measure (default: true)
#sqldump.mondrianschema.ignorecubewithnomeasure=true
# ignore cube with no dimension (default: true)
#sqldump.mondrianschema.ignorecubewithnodimension=true
# preferred level name columns (are used if dimension table has column that matches it)
sqldump.mondrianschema.preferredlevelnamecolumns=label, name
# add all degenerated dimension candidate columns to schema (dafault: false)
#sqldump.mondrianschema.addalldegeneratedimcandidates=true

#
# alter schema suggestions output
#

# outfilepattern may use [schemaname] and [objecttype]
# sqldump.alterschemasuggester.outfilepattern=${outputdir}/alter-schema-suggestions-[schemaname]-[objecttype].sql
sqldump.alterschemasuggester.outfilepattern=${outputdir}/alter-schema-suggestions.sql
sqldump.alterschemasuggester.simplefksonly=true
#sqldump.alterschemasuggester.alterobjectsfromschemas=SCHEMA_X, SCHEMA_Y


############
#
# connection/driver info 
#

# to select a different connection properties prefix
#sqldump.connpropprefix=sqldump.conn1
# example:
#environment=production
#sqldump.connpropprefix=sqldump.${environment}
#sqldump.production.dburl=jdbc:h2:~/production

#
# reads username/password from console or gui
#
#sqldump.askforusername=true
#sqldump.askforpassword=true
#sqldump.askforusernamegui=true
#sqldump.askforpasswordgui=true

# executes query at connection initialization
#sqldump.initsql=set session LC_NUMERIC to 'English'

# MS Access
#sqldump.driverclass=sun.jdbc.odbc.JdbcOdbcDriver
#sqldump.dburl=jdbc:odbc:Driver={MicroSoft Access Driver (*.mdb)};DBQ=Northwind.mdb
# accdb (not fully supported):
#sqldump.dburl=jdbc:odbc:Driver={Microsoft Access Driver (*.mdb, *.accdb)};DBQ=Northwind.accdb

# Oracle
#sqldump.driverclass=oracle.jdbc.OracleDriver
#sqldump.dburl=jdbc:oracle:thin:@<url>:<port>:<sid>
#sqldump.user=scott
#sqldump.password=tiger

# PostgreSQL
#sqldump.driverclass=org.postgresql.Driver
#sqldump.dburl=jdbc:postgresql://<host>/<db>
#sqldump.user=
#sqldump.password=

# Derby Embedded
#sqldump.driverclass=org.apache.derby.jdbc.EmbeddedDriver
#sqldump.dburl=jdbc:derby:<db-path>
#sqldump.user=sa
#sqldump.password=sa

# HSQLDB In-Process
#sqldump.driverclass=org.hsqldb.jdbc.JDBCDriver
#sqldump.dburl=jdbc:hsqldb:file:<db-path>
#sqldump.user=sa
#sqldump.password=sa

# MySQL
#sqldump.driverclass=com.mysql.jdbc.Driver
#sqldump.dburl=jdbc:mysql://<host>:<port>/<database>
#sqldump.user=
#sqldump.password=

# SQLite
#sqldump.driverclass=org.sqlite.JDBC
#sqldump.dburl=jdbc:sqlite:</path/to/sqlite.db>

# H2
#sqldump.driverclass=org.h2.Driver
#sqldump.dburl=jdbc:h2:~/test

# Monet DB
#sqldump.driverclass=nl.cwi.monetdb.jdbc.MonetDriver
#sqldump.dburl=jdbc:monetdb://localhost/demo
#sqldump.user=monetdb
#sqldump.password=monetdb

# Connection from datasource/jndi/pool
#sqldump.datasource=jdbc/LocalTestDB
