
###############################################################################
# sqldump config file                                                         #
# https://bitbucket.org/tbrugz/sqldump                                        #
###############################################################################

# property values can use fixed/constant property:
# ${propfilebasedir} - contains the properties file directory

# includes other properties files
#
# ${propfilebasedir}, ${user.home} & ${user.dir} may be used
#
#@includes = other.properties, yet-another.properties, ${propfilebasedir}/prop-x.properties, ${user.home}/my.properties, ${user.dir}/xyz.properties

# just a variable for using ahead - name has no special meaning 
outputdir=output

# environment variables may be used with 'env.' prefix
# - linux example:
#outputdir=${env.HOME}/output
# - windows example
#outputdir=${env.USERPROFILE}/output

# property may be defined with COALESCE-like function (returns the first of its arguments that is not null)
#outputdir=${env.HOME|env.USERPROFILE|propx}/output

###

#
# sqldump main properties (grab/dump classes)
#

# grabclass can be: JDBCSchemaGrabber, JAXBSchemaXMLSerializer, SchemaSerializer,
#    CastorSchemaXMLSerializer, JSONSchemaSerializer
sqldump.grabclass=JDBCSchemaGrabber

# Processors & Dumpers should be added to 'sqldump.processingclasses' and will be executed in the defined order
# dumpclasses can be: 
#    SchemaModelScriptDumper, FKScriptDumper, graph.Schema2GraphML,
#    SchemaSerializer, JAXBSchemaXMLSerializer, CastorSchemaXMLSerializer,
#    xtradumpers.AlterSchemaSuggester, mondrianschema.MondrianSchemaDumper,
#    JSONSchemaSerializer
# processingclasses can be: graph.ResultSet2GraphML, 
#    DataDump, SQLQueries, CascadingDataDump, 
#    SQLDialectTransformer, xtraproc.ModelSQLIdTransformer, SchemaModelTransformer,
#    mondrianschema.Olap4jConnector, mondrianschema.MondrianSchemaValidator, mondrianschema.Olap4jMDXQueries,
#    mondrianschema.MondrianSchema2GraphProcessor
#    SendMail, xtradumpers.DropScriptDumper
#    tbrugz.sqldump.sqlrun.processor.SQLRunProcessor
#    xtraproc.StatsProc
sqldump.processingclasses=SchemaModelScriptDumper, JAXBSchemaXMLSerializer, graph.Schema2GraphML, graph.ResultSet2GraphML

# if sqldump should fail on error (default is true)
sqldump.failonerror=true|false

#
# monitoring: jmx mbean
# (option to create a JMX Managed Bean, default is false)
#
sqldump.jmx.create-mbean=false

#
# delete files
# option option to delete initial output dir contents (except special hidden files (unix dotfiles, eg: .svn, .git, .hg) and files that starts with "_")
# ('delete files' wil be added as a processor after grabclass & before first processingclass)
#
sqldump.deleteregularfilesfromdir=${outputdir}


###

#
# schema grab properties (JDBCSchemaGrabber) (defaults below)
#

#sqldump.schemagrab.tables=true
#sqldump.schemagrab.pks=true
#sqldump.schemagrab.fks=true
#sqldump.schemagrab.exportedfks=false
#sqldump.schemagrab.grants=false
## sqldump.dbspecificfeatures.grabindexes may be used instead of sqldump.doschemadump.indexes (really better performance on oracle)
#sqldump.schemagrab.indexes=false
#sqldump.schemagrab.proceduresandfunctions=true
# Table types to grab. Any enum of tbrugz.sqldump.dbmodel.TableType (like TABLE, SYNONYM, SYSTEM_TABLE, VIEW, MATERIALIZED_VIEW, EXTERNAL_TABLE)
# default is null (meaning: all)
#sqldump.schemagrab.tabletypes=TABLE, VIEW
#sqldump.schemagrab.ignoretableswithzerocolumns=true
#sqldump.schemagrab.setconnectionreadonly=false
# grab metadata info - and include in model
#sqldump.schemagrab.metadata=false

# schema info: domain tables
# (used by AlterSchemaSuggester, but may be have other uses, like 'sqldump.datadump.tables', 'sqldump.graphmldump.nodestereotypecontains.domaintable')
#sqldump.schemainfo.domaintables=DOMAIN_TABLE_X, DOMAIN_TABLE_Y

# schemas to grab
sqldump.schemagrab.schemas=<DW, SA, EMPDEPT>

# filter tables by name
sqldump.schemagrab.tablefilter=TABLE_X, TABLE_Y

# exclude table filters (regex), splitted by "|"
#sqldump.schemagrab.tablename.excludes=EMPT.* | .*_OLD | ZZZ_.*
# exclude object filters
#sqldump.schemagrab.objectname.excludes=BIN\\$.*

# recursive grab of tables based on FKs, defaults below (default maxlevel is null)
#sqldump.schemagrab.recursivegrabbasedonfks=false
#sqldump.schemagrab.recursivegrabbasedonfks.exportedfks=false
#sqldump.schemagrab.recursivegrabbasedonfks.maxlevel=3
#sqldump.schemagrab.recursivegrabbasedonfks.deep=false

#
# 'sqldump.fromdbid' selects which specificgrabclass to use, column type conversion, ... 
#
#sqldump.fromdbid=oracle
# autodetect database product
sqldump.fromdbid.autodetect=true

#
# to sql-dialect transform (table columns): add 'SQLDialectTransformer' to 'sqldump.processingclasses'
# & select target sql-dialect (prop 'sqldump.todbid') 
# or set 'sqldump.schematransform.toansi' to 'true'
#
#sqldump.todbid=pgsql
#sqldump.schematransform.toansi=true|false


###

#
# script dump properties
#
# for schema script dumping add 'SchemaModelScriptDumper' to 'sqldump.processingclasses' property

#
# filepattern output props
# patterns that may be used: [schemaname], [objecttype], [objectname]
#
sqldump.schemadump.outputfilepattern=${outputdir}/[schemaname]_[objecttype].sql
sqldump.schemadump.outputobjectwithreferencingtable=grant, index
#sqldump.schemadump.outputfilepattern.maptype.PROCEDURE=EXECUTABLE
#sqldump.schemadump.outputfilepattern.maptype.TRIGGER=EXECUTABLE
#sqldump.schemadump.outputfilepattern.maptype.FUNCTION=EXECUTABLE
#sqldump.schemadump.outputfilepattern.bytype.EXECUTABLE=${outputdir}/EXECUTABLEs.sql

# to directly dump scripts to another database (use instead of 'sqldump.schemadump.outputfilepattern')
#sqldump.schemadump.output.connpropprefix=sqldump.conn-v2

# other sqldump.schemadump.outputfilepattern examples:
#sqldump.schemadump.outputfilepattern=output/[schemaname]/[objecttype]/[objectname].sql
#sqldump.schemadump.outputfilepattern=output/[objecttype]/[schemaname]_[objectname].sql
#sqldump.schemadump.outputobjectwithreferencingtable=grant, index, fk, trigger

# other outputfilepattern by object types. 'sqldump.schemadump.outputobjectwithreferencingtable' takes precedence
#sqldump.schemadump.outputfilepattern.bytype.TABLE=output/TABLE.sql
#sqldump.schemadump.outputfilepattern.bytype.VIEW=output/VIEW.sql
#sqldump.schemadump.outputfilepattern.bytype.INDEX=output/INDEX.sql
#sqldump.schemadump.outputfilepattern.bytype.EXECUTABLE=output/EXECUTABLE.sql
#sqldump.schemadump.outputfilepattern.bytype.TRIGGER=output/TRIGGER.sql
#sqldump.schemadump.outputfilepattern.bytype.SEQUENCE=output/SEQUENCE.sql
#sqldump.schemadump.outputfilepattern.bytype.SYNONYM=output/SYNONYM.sql
#sqldump.schemadump.outputfilepattern.bytype.GRANT=output/GRANTS.sql
#sqldump.schemadump.outputfilepattern.bytype.FK=output/FK.sql

#sqldump.schemadump.dumpwithschemaname=true
#sqldump.schemadump.dumpsynonymastable=false
#sqldump.schemadump.dumpviewastable=false
#sqldump.schemadump.dumpmaterializedviewastable=false
#sqldump.schemadump.dumpdropstatements=false
#sqldump.schemadump.usecreateorreplace=false
#sqldump.schemadump.fks.atend=true
# defaults below
#sqldump.schemadump.dumpscriptcomments=true
#sqldump.schemadump.dumpremarks=true
#sqldump.schemadump.pks=true
#sqldump.schemadump.fks=true
# order to dump indexes (default is null, which is equal to 'indexname')
#sqldump.schemadump.index.orderby=|indexname|tablename

# sql types to use/ignore column precision (overrides defaults from 'src/dbms-specific.properties')
sqldump.sqltypes.useprecision=FLOAT8
sqldump.sqltypes.ignoreprecision=SMALLINT,BIGINT,INTEGER

# quote all SQL identifiers (defalut: false)
sqldump.schemadump.quoteallsqlidentifiers=false

###

#
# dbms specific features grab (default: false)
#
sqldump.schemagrab.db-specific-features=false|true

# use alternate features class
#sqldump.dbms@<dbid>.specificgrabclass=tbrugz.sqldump.dbmsfeatures.InformationSchemaFeatures
#sqldump.dbms.specificgrabclass=tbrugz.sqldump.dbmsfeatures.InformationSchemaFeatures

# grab dbspecific by type (defaults below)
#sqldump.dbspecificfeatures.grabindexes=true
#sqldump.dbspecificfeatures.grabexecutables=true
#sqldump.dbspecificfeatures.grabviews=true
#sqldump.dbspecificfeatures.grabmaterializedviews=true
#sqldump.dbspecificfeatures.grabtriggers=true
#sqldump.dbspecificfeatures.grabsynonyms=true
#sqldump.dbspecificfeatures.grabsequences=true
# 'extraconstraints' are those besides PK & FK (like unique and check constraints)
#sqldump.dbspecificfeatures.grabextraconstraints=true

# extra dmbs-specific properties (oracle for now)
#sqldump.dbspecificfeatures.sequencestartwithdump=false
# grabbing FKs from unique keys may slower grabbing significantly (but may grab extra info)
#sqldump.dbspecificfeatures.grabfkfromuk=false
#sqldump.dbspecificfeatures.dumpphysicalattributes=true
#sqldump.dbspecificfeatures.dumplogging=true
#sqldump.dbspecificfeatures.dumppartition=true

###

#
# processor: xtraproc.ModelSQLIdTransformer: transforms sql identifiers
# 
# 2 options: toupper, tolower
#
sqldump.proc.sqlidtransformer=tolower|toupper
#sqldump.proc.sqlidtransformer.iddecorator=...
#sqldump.proc.sqlidtransformer.coltypedecorator=...

###

#
# data dump props
#
# for datadumping: add DataDump to 'sqldump.processingclasses'

# types of table to dump (default: all but VIEW, MATERIALIZED_VIEW)
#sqldump.datadump.tabletypes=TABLE, SYNONYM, SYSTEM_TABLE, VIEW, MATERIALIZED_VIEW, EXTERNAL_TABLE

# filter tables to dump data
#sqldump.datadump.tables=TABLE_X, TABLE_Y, TABLE_Z
#sqldump.datadump.tables=${sqldump.schemainfo.domaintables}, TABLE_Z

# order table data by PK (dafault is true)
#sqldump.datadump.orderbypk=true

# ignore tables filter ("|" separated regexes)
#sqldump.datadump.ignoretables=TABLE_ABC.* | TABLE_DEF.*

# dump syntaxes (any of: insertinto, csv, xml, html, json, ffc, updatebypk, blob, turtle, dbktable, rncsv, iidb, deletebypk, simple-ods, jopen-ods, webrowset-single, rowset-ser)
# (for more information, see 'dumpsyntaxes.properties')
sqldump.datadump.dumpsyntaxes=insertinto, csv

# extra syntaxes (full class name or classes in package 'tbrugz.sqldump.datadump') - classes must be in classpath
#sqldump.datadump.xtrasyntaxes=org.example.procutcx.SyntaxX, anotherClass

# default prop: sqldump.datadump.outfilepattern
# patterns that may be used: [tablename], [id] (table/query id), [partitionby], [syntaxfileext]
# sqldump.datadump.outfilepattern=${outputdir}/data/[tablename].[syntaxfileext]
sqldump.datadump.outfilepattern=${outputdir}/data/[tablename][partitionby].[syntaxfileext]

# create empty data files if query returned no data? (default is false)
#sqldump.datadump.createemptyfiles=false|true

# when using partitionby with date column, you may define a dateformat (default below)
#sqldump.datadump.partitionby.dateformat=yyyy-MM-dd

# append to existing files (default is false)
#sqldump.datadump.writeappend=false

# log dump for each X rows, default is 10000 (logger is tbrugz.sqldump.DataDump.datadump-row)
#sqldump.datadump.logeachxrows=100
# log 1st row dumped
#sqldump.datadump.log1strow=true

# charset:: default: UTF-8; options: ISO-8859-1, UTF-16, US-ASCII, ...
#sqldump.datadump.charset=UTF-8

# write (or not) BOM (Byte order mark) - defaults to false
#sqldump.datadump.writebom=true|false|<null>
# write (or not) BOM (Byte order mark) by syntax
#sqldump.datadump.csv.writebom=false|true
#sqldump.datadump.xml.writebom=false|true

# default date format: ''yyyy-MM-dd''
# deprecated property 'sqldump.datadump.dateformat' - use preferably 'sqldump.datadump.<syntax>.dateformat'
#sqldump.datadump.dateformat=''yyyy-MM-dd''
sqldump.datadump.rowlimit=100
sqldump.datadump.orderbypk=true

# SQLUtils properties (defaults below)
#sqldump.sqlutils.strangePrecisionNumericAsInt=true
#sqldump.sqlutils.defaultTypeIsString=true
#sqldump.sqlutils.clobTypeIsString=true

# table-based datadump properties
#sqldump.datadump.TABLE_X.rowlimit=500
#sqldump.datadump.TABLE_X.where=NAME like 'Bill %'
#sqldump.datadump.TABLE_Y.columns=id, name
#sqldump.datadump.TABLE_Y.order=TYPE, NAME

# syntax-specific props: sqldump.datadump.outfilepattern.syntax@<syntaxid>
#sqldump.datadump.outfilepattern.syntax@insertinto=${outputdir}/data_[tablename].sql
#sqldump.datadump.outfilepattern.syntax@csv=${outputdir}/data_[tablename].csv
#sqldump.datadump.outfilepattern.syntax@json=${outputdir}/data_[tablename].json
#sqldump.datadump.outfilepattern.syntax@xml=${outputdir}/data_[tablename].xml


#
# InsertInto syntax props
#
sqldump.datadump.insertinto.withcolumnnames=true
sqldump.datadump.insertinto.dateformat='DATE '''yyyy-MM-dd'''
# oracle TO_DATE syntax: 'TO_DATE('''yyyy-MM-dd''',''YYYY-MM-DD'')'
# sqldump.datadump.insertinto.dateformat='TO_DATE('''yyyy-MM-dd''',''YYYY-MM-DD'')'
sqldump.datadump.insertinto.dumpcursors=false
# [tablename] is replaced by table name in header & footer 
#sqldump.datadump.insertinto.header=--header [tablename]
#sqldump.datadump.insertinto.footer=--footer [tablename]
# dumps many rows in one sql statement (default is false)
#sqldump.datadump.insertinto.compactmode=false
# quote all sql identifiers (table & column names - default is false)
#sqldump.datadump.insertinto.quotesql=false
# dump schema name (default is false)
#sqldump.datadump.insertinto.dumpschema=false|true

#
# CSV syntax props
#
sqldump.datadump.csv.tablenameheader=false
sqldump.datadump.csv.columnnamesheader=true
sqldump.datadump.csv.recorddelimiter=\r\n
sqldump.datadump.csv.columndelimiter=;
sqldump.datadump.csv.enclosing="
sqldump.datadump.csv.nullvalue=
sqldump.datadump.csv.floatlocale=en
sqldump.datadump.csv.floatformat=###0.000

# JSON syntax props
sqldump.datadump.json.dateformat='"'yyyy-MM-dd'T'HH:mm:ss.SSSZ'"'
# add metadata element
sqldump.datadump.json.add-metadata=false|true
# metadata's element name
#sqldump.datadump.json.metadata-element=@metadata
# data's element name (defaults to table name)
#sqldump.datadump.json.data-element=data
# JSONP - callback function (see http://json-p.org/)
#sqldump.datadump.json.callback=my_callback

# XML syntax props
sqldump.datadump.xml.dumpnullvalues=true|false
sqldump.datadump.xml.nullvalue=nil
sqldump.datadump.xml.rowelement=myrow
sqldump.datadump.xml.dumprowelement=true|false
sqldump.datadump.xml.rowelement4table@<table-name>=table_x_rows
sqldump.datadump.xml.dumprowelement4table@<table-name>=true|false
sqldump.datadump.xml.dumpheader4innertables=ALWAYS | IFHASDATA | NEVER
sqldump.datadump.xml.dumptablenameasrowtag=false|true
# escape XML special chars (inherited by HTML syntax)
sqldump.datadump.xml.escape=false | true
sqldump.datadump.xml.escape4table@<table-name>=false | true
sqldump.datadump.xml.escapecols4table@<table-name>=COL1, COL2
sqldump.datadump.xml.noescapecols4table@<table-name>=COL3, COL4

# HTML syntax props
sqldump.datadump.html.nullvalue=
sqldump.datadump.html.prepend=<!DOCTYPE html>\n<html><head><link rel="stylesheet" type="text/css" href="sqldump.css" /></head><body>\n
sqldump.datadump.html.append=\n</body></html>
sqldump.datadump.html.add-caption=false|true
sqldump.datadump.html.style.numeric-align-right=false|true
# prepend/append on 'inner tables' - table inside table
sqldump.datadump.html.xpend-inner-table=false|true
#TODO? sqldump.datadump.html.floatlocale

# FFC (formatted fixed column) syntax props
#sqldump.datadump.ffc.linegroupsize=30
#sqldump.datadump.ffc.columndelimiter=\ ;\ 
#sqldump.datadump.ffc.nullvalue=<null>
#sqldump.datadump.ffc.showcolnames=false
#sqldump.datadump.ffc.showcolnameslines=false
sqldump.datadump.ffc.floatlocale=pt

# BLOB syntax props
# blobs may work better if are the first in 'sqldump.datadump.dumpsyntaxes' prop
sqldump.datadump.blob.outfilepattern=${outputdir}/data/[tablename]_[columnname]_[rowid].[syntaxfileext]
# columns-to-dump may be defined for each table/query name - if not defined, only BLOBs will be dumped
#sqldump.datadump.blob.columns2dump@<table-name>=COL1, COL2
# outfilepattern may be defined for each table/query name ; patterns like [col:<column-name>] may be used 
#sqldump.datadump.blob.outfilepattern@<table-name>=${outputdir}/data/[tablename]_[col:<column-x>].[syntaxfileext]

# Turtle syntax props
# base URL for 'RDF-based' syntaxes
#sqldump.rdf.base=http://example.org/
# defaults below
#sqldump.datadump.turtle.keycolseparator=;
#sqldump.datadump.turtle.keyincludescolname=true
# appends constant string to keys (default is null)
#sqldump.datadump.turtle.keyappend=#this

# iidb (InsertIntoDatabase) syntax props
#
# '.connpropprefix' defines properties for connecting into another database for data export
#sqldump.datadump.iidb.connpropprefix=iidb.h2connection
# properties for export-database connection
#iidb.h2connection.driverclass=org.h2.Driver
#iidb.h2connection.dburl=jdbc:h2:~/export-db
# extra props (defaults below)
#sqldump.datadump.iidb.autocommit=false
#sqldump.datadump.iidb.batchmode=false
#sqldump.datadump.iidb.commitsize=100
# if '.dropcreatetables' is set to true, iidb attempts to drop & create tables - use with caution
#sqldump.datadump.iidb.dropcreatetables=false

#
# SimpleODS (simple-ods) - Apache ODF Toolkit, Simple API - http://incubator.apache.org/odftoolkit/simple/
#
# '.outfilepattern' must be set
#sqldump.datadump.simple-ods.outfilepattern=${outputdir}/data/[tablename].[syntaxfileext]

#
# jOpenDocument-ODS (jopen-ods) - http://www.jopendocument.org/
#
# '.outfilepattern' must be set
#sqldump.datadump.jopen-ods.outfilepattern=${outputdir}/data/[tablename].[syntaxfileext]


###

#
# sql queries
#
# for sqlqueries dumping: use 'sqldump.processingclasses=SQLQueries'

#sqldump.queries=table_x, q2, partitiontest, qpivot
# extra options for sqlqueries [defaults at first]
#sqldump.queries.addtomodel=false|true
#sqldump.queries.runqueries=true|false
#sqldump.queries.schemaname=MY_QUERIES_SCHEMA
# grab columns info (name, type) from prepared statement's (sql) metadata (default is false)
#sqldump.queries.grabcolsinfofrommetadata=false|true

#sqldump.query.table_x.sql=\
#select * from TABLE_X where abc=?
#sqldump.query.table_x.schemaname=TABLEX_SCHEMA
#sqldump.query.table_x.name=TABLE_X
#sqldump.query.table_x.rowlimit=10000
#sqldump.query.table_x.keycols=ID
#sqldump.query.table_x.param.1=3
# query dump syntaxes takes precedence over default datadump syntaxes (sqldump.datadump.dumpsyntaxes)
#sqldump.query.table_x.dumpsyntaxes=csv, json, xml
#sqldump.query.table_x.remarks=This table stores info about...
# roles with select grant
#sqldump.query.table_x.roles=user_x|admin

#sqldump.query.q2.sqlfile=c:/xxx/query2.sql
#sqldump.query.q2.name=query_2
#sqldump.query.q2.keycols=ID, PARENT_ID
#sqldump.query.q2.cols=NAME:VARCHAR2, DESCRIPTION, TYPE_ID:INTEGER
# replaces '${sqldump.query.replace.<1-n>}' in sql statement by value of 'sqldump.query.<query-id>.replace.<1-n>'
#sqldump.query.q2.replace.1=2011
#sqldump.datadump.outfilepattern.id@q2=${outputdir}/data_q2_[tablename].[syntaxfileext]

#sqldump.query.partitiontest.name=partition_test
#sqldump.query.partitiontest.sql=\
#  select group_id, id, name\
#  from partition_table_test
#sqldump.query.partitiontest.partitionby=_[col:group_id]
# multiple partitionby-patterns may be defined
#sqldump.query.partitiontest.partitionby=| _[col:group_id] | _[col:group_id]_[col:id]
#sqldump.datadump.outfilepattern.id@partitiontest.syntax@csv=${outputdir}/data_partitiontest_csv_[tablename][partitionby].csv
#sqldump.datadump.outfilepattern.id@partitiontest=${outputdir}/data_partitiontest_[tablename][partitionby].[syntaxfileext]

#sqldump.query.qpivot.name=pivot_query
#sqldump.query.qpivot.sql=\
#  select id, category, year, measure\
#  from pivot_table_test
#sqldump.query.qpivot.rsdecoratorfactory=pivot.PivotRSFactory
#sqldump.query.qpivot.rsdecorator.colsnottopivot=id, category
#sqldump.query.qpivot.rsdecorator.colstopivot=year


###

#
# cascading data dump
#
# for cascading data dumping: use 'sqldump.processingclasses=CascadingDataDump'
#
# determine tables used to start cascading dump
#sqldump.cascadingdd.starttables=TABLE_A, TABLE_B
# tables that will not be dumped
#sqldump.cascadingdd.stoptables=TABLE_X
# do not follow exported keys for these tables - no export! (even if they are start tables)
#sqldump.cascadingdd.noexporttables=TABLE_Z, TABLE_Y
# filters for start tables dump
#sqldump.cascadingdd.filter@TABLE_A=[tablename].year in (2011) and [tablename].id_class in (25, 35, 39)
# props to change cascading dump behavior (defaults below)
#sqldump.cascadingdd.exportedkeys=false
#sqldump.cascadingdd.orderbypk=true


###

#
# serialization & xml serialization
#
sqldump.serialization.outfile=${outputdir}/schemaModel.ser
sqldump.serialization.infile=${outputdir}/schemaModel.ser
sqldump.serialization.inresource=/schemaModel.ser

sqldump.xmlserialization.jaxb.outfile=${outputdir}/schemaModel.jaxb.xml
sqldump.xmlserialization.jaxb.infile=${outputdir}/schemaModel.jaxb.xml
sqldump.xmlserialization.jaxb.inresource=/schemaModel.jaxb.xml

sqldump.jsonserialization.outfile=${outputdir}/schemaModel.json
sqldump.jsonserialization.infile=${outputdir}/schemaModel.json
sqldump.jsonserialization.inresource=/schemaModel.json

sqldump.xmlserialization.castor.outfile=${outputdir}/schemaModel.castor.xml
sqldump.xmlserialization.castor.infile=${outputdir}/schemaModel.castor.xml


###

#
# graphML output
#
# for graphML schema dumping use 'sqldump.processingclasses=graph.Schema2GraphML'

sqldump.graphmldump.outputfile=${outputdir}/db-schema.graphml
# dump format (default is DumpSchemaGraphMLModel)
#sqldump.graphmldump.dumpformatclass=DumpSchemaGraphMLModel|DumpGXLModel|DumpDotModel

#sqldump.graphmldump.showschemaname=true|false
#sqldump.graphmldump.showconstraints=false|true
#sqldump.graphmldump.showindexes=false|true
#
# default edgelabel: NONE
#sqldump.graphmldump.edgelabel=FK|FKANDCOLUMNS|COLUMNS|FKCOLUMNS|PKCOLUMNS|NONE
#sqldump.graphmldump.nodeheightbycolsnumber=true|false

# equals-based stereotypes, splitted by "," - stereotype must be defined in a graphml-snippets properties file
#sqldump.graphmldump.nodestereotypecontains.abc=TABLE_X, TABLE_Y
# 'domaintable' is a special stereotype defined in 'graphml-snippets.properties'
#sqldump.graphmldump.nodestereotypecontains.domaintable=${sqldump.schemainfo.domaintables}

# regex-based stereotypes, splitted by "|" - stereotype must be defined in a graphml-snippets properties file
#sqldump.graphmldump.nodestereotyperegex.zzz=ZZZ_.* | .*_ZZZ_.*

#
# include, or not, stereotype 'classes' - defaults below
#sqldump.graphmldump.addtabletypestereotype=true
#sqldump.graphmldump.addschemastereotype=false
# vertex types: root, leaf, connected, disconnected
#sqldump.graphmldump.addvertextypestereotype=false
#
# snippets file (non-default)
#sqldump.graphmldump.snippetsfile=graphml-snippets-simple.properties


###

#
# graphmlqueries processor
#
# for schema script dumping add 'graph.ResultSet2GraphML' to 'sqldump.processingclasses' property

# queries to generate graph
sqldump.graphmlqueries=graph1

# mandatory columns for 'edge-sql': SOURCE, TARGET
# optional columns for 'edge-sql': EDGE_TYPE, EDGE_WIDTH
sqldump.graphmlquery.graph1.sql=select SOURCE, TARGET, EDGE_TYPE, EDGE_WIDTH from xxx
#sqldump.graphmlquery.graph1.sqlfile=/home/xxx/edges.sql
# you may use a 'nodesql' query or 2 extra columns in 'edge/default' sql: SOURCE_TYPE, TARGET_TYPE

# mandatory columns for 'node-sql', if present: OBJECT, OBJECT_LABEL
# optional columns for 'node-sql': OBJECT_TYPE, OBJECT_X, OBJECT_Y, OBJECT_WIDTH, OBJECT_HEIGHT, OBJECT_DESC
sqldump.graphmlquery.graph1.nodesql=select OBJECT, OBJECT_TYPE, OBJECT_LABEL, OBJECT_DESC from xxx
#sqldump.graphmlquery.graph1.nodesqlfile=/home/xxx/nodes.sql

sqldump.graphmlquery.graph1.outputfile=${outputdir}/graph1.graphml
# dump format (default is DumpResultSetGraphMLModel)
#sqldump.graphmlquery.graph1.dumpformatclass=DumpResultSetGraphMLModel|DumpGXLModel|DumpDotModel

# extra properties for initial/final (source/sink) node types
#sqldump.graphmlquery.q2.initialnodelabel=dashed
#sqldump.graphmlquery.q2.finalnodelabel=dashed
sqldump.graphmlquery.q2.initialorfinalnodelabel=5.0

#sqldump.graphmlquery.graph1.snippetsfile=graphml-snippets-simple.properties


###

#
# mondrian schema dumper 
#
# for mondrian schema dumping add 'mondrianschema.MondrianSchemaDumper' to 'sqldump.processingclasses' property

sqldump.mondrianschema.outfile=${outputdir}/mondrian-schema.xml
# mondrian schema name
sqldump.mondrianschema.schemaname=<schema-name>
# add dimension for each hierarchy
sqldump.mondrianschema.adddimforeachhierarchy=true
# add sql identifier string decorator
sqldump.mondrianschema.sqliddecorator=toupper
# set 'hasAll' property (for all dimensions, default is true)
#sqldump.mondrianschema.hierarchyhasall=false
# determine fact tables (otherwise fact tables are inferred)
sqldump.mondrianschema.facttables=table_fact1, table_fact2
# add fact tables (besides inferred fact tables)
sqldump.mondrianschema.xtrafacttables=table_fact_x
# default measure aggregators (real default is only 'sum')
sqldump.mondrianschema.defaultaggregators=sum, count
# inores dimension tables
sqldump.mondrianschema.ignoredims=dim_table_xx, dim_table_yy
# adds fact-count measure for every cube
sqldump.mondrianschema.factcountmeasure=factcount
# ignore measure columns that belongs do PK (default is true)
#sqldump.mondrianschema.ignoremeasurecolumnsbelongingtopk=false
# determine measure columns based on regex (global)
sqldump.mondrianschema.measurecolsregex=measure_.* | amount_.*
# define level name based on patterns (default is '[tablename]')
# patterns: [tablename], [pkcolumn], [fkcolumn]
sqldump.mondrianschema.levelname.pattern=[tablename]/[fkcolumn]
# max snowflake level (default is 'no limit', i.e.: -1)
sqldump.mondrianschema.snowflake.maxlevel=4
# to set a different measures caption
sqldump.mondrianschema.measuresCaption=Medidas
# hierarchy's 'All' member caption pattern - patterns [dimcaption] & [dimname] may be used
sqldump.mondrianschema.hier.allMemberCaptionPattern=(all [dimname])
# by-hierarchy 'All' member caption
sqldump.mondrianschema.hier@<hier-name>.allMemberCaption=all products

# define level name column
sqldump.mondrianschema.level@<dim_table>.levelnamecol=<level_name_column>
# define levels for dimension table (for classic star schema: multiple levels in one table; does not include lowest (PK/UK) level)
sqldump.mondrianschema.level@<dim_table>.levels=<level_column_1>:<level_name_column_1>, <level_column_2>:<level_name_column_2>
# define parent levels for dimension table (for classic star schema: multiple levels in one table; includes lowest (PK/UK) level)
sqldump.mondrianschema.level@<dim_table>.parentLevels=<level_column_1>:<level_name_column_1>, <level_column_2>:<level_name_column_2>
# add lower levels as separated hierarchies for table (experimental)
sqldump.mondrianschema.level@<dim_table>.addLowerLevelsAsDistinctHiers=true
# determine measure columns (otherwise they are inferred)
sqldump.mondrianschema.cube@<fact_table>.measurecols=amount_x, measure_y
# determine measure columns based on regex
sqldump.mondrianschema.cube@<fact_table>.measurecolsregex=amount_.*, measure_.y
# degenerated dimension columns
sqldump.mondrianschema.cube@<fact_table>.degeneratedims=<column_1>, <column_2>:<optional_level_name_column_2>
# override default aggregators for a cube
sqldump.mondrianschema.cube@<fact_table>.aggregators=sum, count
# adds extra measures
sqldump.mondrianschema.cube@<fact_table>.addmeasures=<column>:<aggregator>[:<label>];<another-measure>
# extra dimension tables/columns (when there is no FK)
sqldump.mondrianschema.table@<table>.xtrafk=<fk_column_1>:<schema_dim_table_1>.<dim_table_1>:<dim_column_1>, (...)
# level's internal type (Legal values: {int, long, Object, String})
sqldump.mondrianschema.table@<table>.column@<column>.internalType=long

# ignore cube with no measure (default: true)
#sqldump.mondrianschema.ignorecubewithnomeasure=true
# ignore cube with no dimension (default: true)
#sqldump.mondrianschema.ignorecubewithnodimension=true
# preferred level name columns (are used if dimension table has column that matches it)
sqldump.mondrianschema.preferredlevelnamecolumns=label, name
# add all degenerated dimension candidate columns to schema (dafault: false)
#sqldump.mondrianschema.addalldegeneratedimcandidates=true

# cube, dinmension & level captions
sqldump.mondrianschema.cube@<cube-name>.caption=My Cube Name
sqldump.mondrianschema.dim@<dimension-name>.caption=My Dimension
sqldump.mondrianschema.level@<level-name>.caption=Level Name


#
# mondrian schema validator
#
# jars that are needed in classpath for mondrian validation:
# olap4j.jar, mondrian.jar, 
# commons-collection.jar, commons-dbcp.jar, commons-pool.jar, commons-vfs.jar, commons-math.jar,
# eigenbase-properties.jar, eigenbase-resgen.jar & eigenbase-xom.jar

# schema file to be validated - if not defined, prop 'sqldump.mondrianschema.outfile' is used
sqldump.mondrianvalidator.schemafile=${outputdir}/mondrian-schema.xml
# validator may be used with mondrian schema dumper - prop below must be true (default is false)
sqldump.mondrianschema.validateschema=true
# may be used as 'standalone' - add 'mondrianschema.MondrianSchemaValidator' to processingclasses
#sqldump.processingclasses=mondrianschema.MondrianSchemaValidator


#
# olap4j mdx queries datadump - 'mondrianschema.Olap4jMDXQueries'
#
# depends on previous 'mondrianschema.Olap4jConnector' processor
# MDX queries ids
#sqldump.mdxqueries.ids=q1
# datadump syntaxes
#sqldump.mdxqueries.dumpsyntaxes=csv, html
#
# use olap4j's RectangularCellSetFormatter (default is false, i.e., using CellSetResultSetAdapter + DataDump processor) 
#sqldump.mdxqueries.use-cellset-formatter=true
# output file pattern ; pattern [tablename] may be used (if using RectangularCellSetFormatter)
# (if not pattern 'sqldump.datadump.outfilepattern' will be used)
#sqldump.mdxqueries.outfilepattern=${outputdir}/[tablename].txt
# RectangularCellSetFormatter's compact mode (false/true) 
#sqldump.mdxqueries.x-compactmode=true
#
# MDX queries
#sqldump.mdxqueries.q1.query=SELECT NON EMPTY {Hierarchize({[Measures].[factcount]})} ON COLUMNS, \
#	NON EMPTY {Hierarchize({{[DIM_OS].[OS_FAMILY].Members}, {[DIM_OS].[DIM_OS].Members}})} ON ROWS FROM [HTTP_LOGS]
#sqldump.mdxqueries.q1.name=query1


#
# MondrianSchema2GraphProcessor
#
#sqldump.mondrianschema2graph.infile=mondrian/demo/FoodMart.xml
# if '.infile' is not defined, 'sqldump.mondrianschema.outfile' is used
# e.g: sqldump.mondrianschema.outfile=${outputdir}/foormartschema-new.xml
#sqldump.mondrianschema2graph.outfile=${outputdir}/foodmart.graphml
# use simple XSL (no nodes for hierarchies) - default is false
#sqldump.mondrianschema2graph.xsl.simple=true


###

#
# schema model transform: add 'SchemaModelTransformer' to 'sqldump.processingclasses'
#
# removes schema name from all objects
#sqldump.modeltransform.removeschemaname=false
# removes tables with referencing FKs
#sqldump.modeltransform.removetableswithfks=TABLE_A, TABLE_B
# remove views' definitions (sql query)
#sqldump.modeltransform.remove-views-definitions=true
# removes FKs by name
#sqldump.modeltransform.removefksbyname=FK1, EMP_DEPT_FK, ...
# adds xtra FKs to table <fktable>
#sqldump.modeltransform.table@<fktable>.xtrafk=<fkcolumn-1>[,<fkcolumn-2>...]:[<schema>.]<pktable>:<pkcolumn-1>[,<pkcolumn-2>...][:<fk-name>] [;(...)]
# transform column type 
#sqldump.modeltransform.columntype@<from>=<to>
#sqldump.modeltransform.columntype@DATE=TIMESTAMP # ex: from DATE to TIMESTAMP


###

#
# alter schema suggestions output
#

# outfilepattern may use [schemaname] and [objecttype]
# sqldump.alterschemasuggester.outfilepattern=${outputdir}/alter-schema-suggestions-[schemaname]-[objecttype].sql
sqldump.alterschemasuggester.outfilepattern=${outputdir}/alter-schema-suggestions.sql
sqldump.alterschemasuggester.simplefksonly=true
#sqldump.alterschemasuggester.alterobjectsfromschemas=SCHEMA_X, SCHEMA_Y

###

#
# drop script dumper (xtradumpers.DropScriptDumper)
#
# outfilepattern may use [schemaname] and [objecttype] patterns
sqldump.dropscriptdumper.outfilepattern=${outputdir}/drop_[schemaname]_[objecttype].sql

###

# statistics processor: xtraproc.StatsProc
# default output is <stdout>

# counts the number of rows from each table in the model (select count(*) from <xxx>)
#sqldump.statsproc.counts-by-table=false|true
#sqldump.statsproc.counts-by-table.outfilepattern=${outputdir}/stats.txt
# order of keys: 'name' (default) or 'value'
#sqldump.statsproc.counts-by-table.x-order=name

# grabs statistics from all columns in the model
# - count_all | count_non_null | count_null | cardinality | selectivity
#sqldump.statsproc.stats-by-column=false|true
#sqldump.statsproc.stats-by-column.outfilepattern=${outputdir}/stats-by-column.txt

###

#
# sqlrun processor (tbrugz.sqldump.sqlrun.processor.SQLRunProcessor)
#
# loads properties into SQLRun and execute statements & importers
#
# notable differences from running SQLRun from command line:
# - does not create new connection (uses the existing one)
# - does not process command line arguments
#
# see: sqlrun.template.properties

############
#
# connection/driver info 
#

# to select a different connection properties prefix
#sqldump.connpropprefix=sqldump.conn1
# example:
#environment=production
#sqldump.connpropprefix=sqldump.${environment}
#sqldump.production.dburl=jdbc:h2:~/production

#
# reads username/password from console or gui
#
#sqldump.askforusername=true
#sqldump.askforpassword=true
#sqldump.askforusernamegui=true
#sqldump.askforpasswordgui=true

# executes query at connection initialization
#sqldump.initsql=set session LC_NUMERIC to 'English'

# MS Access
#sqldump.driverclass=sun.jdbc.odbc.JdbcOdbcDriver
#sqldump.dburl=jdbc:odbc:Driver={MicroSoft Access Driver (*.mdb)};DBQ=Northwind.mdb
# accdb (not fully supported):
#sqldump.dburl=jdbc:odbc:Driver={Microsoft Access Driver (*.mdb, *.accdb)};DBQ=Northwind.accdb

# Oracle
#sqldump.driverclass=oracle.jdbc.OracleDriver
#sqldump.dburl=jdbc:oracle:thin:@<url>:<port>:<sid>
#sqldump.user=scott
#sqldump.password=tiger
## 'initsql' example
#sqldump.initsql=alter session set nls_date_format='yyyy-mm-dd hh24:mi:ss'

# - OracleFeatures (default features class - tbrugz.sqldump.dbmsfeatures.OracleFeatures) properties:
# use DBA_* metadata tables/views (default is true) - if false uses ALL_* tables/views
#sqldump.dbms.oracle.use-dba-metaobjects=true|false
# use table DBA_TRIGGERS (default is true) - if false uses ALL_TRIGGERS
#sqldump.dbms.oracle.trigger.use-dba-triggers=true|false
# Oracle's alternative feature classes:
# - OracleDbmsMetadataFeatures: uses mostly DBMS_METADATA.GET_DDL to grab database objects' metadata
#sqldump.dbms.specificgrabclass=tbrugz.sqldump.dbmsfeatures.OracleDbmsMetadataFeatures
# - OracleFeaturesLite: does not create decorator for DatabaseMetaData (see OracleDatabaseMetaData) - uses jdbc driver's plain methods
#sqldump.dbms.specificgrabclass=tbrugz.sqldump.dbmsfeatures.OracleFeaturesLite
# uses SimpleDatabaseMetadata (default is false)
#sqldump.dbms.oraclelite.use-simple-dbmd=false|true

# PostgreSQL
# info: https://jdbc.postgresql.org/
#sqldump.driverclass=org.postgresql.Driver
#sqldump.dburl=jdbc:postgresql://<host>/<db>
#sqldump.user=
#sqldump.password=

# Derby Embedded
#info: https://db.apache.org/derby/docs/10.12/devguide/rdevdvlp22102.html
#sqldump.driverclass=org.apache.derby.jdbc.EmbeddedDriver
#sqldump.dburl=jdbc:derby:<db-path>
#sqldump.user=sa
#sqldump.password=sa

# Derby Client
#sqldump.driverclass=org.apache.derby.jdbc.ClientDriver
#sqldump.dburl=jdbc:derby://localhost:1527/<db>

# HSQLDB In-Process
#sqldump.driverclass=org.hsqldb.jdbc.JDBCDriver
#sqldump.dburl=jdbc:hsqldb:file:<db-path>
#sqldump.user=sa
#sqldump.password=sa

# MySQL
#sqldump.driverclass=com.mysql.jdbc.Driver
#sqldump.dburl=jdbc:mysql://<host>:<port>/<database>
#sqldump.user=
#sqldump.password=

# MariaDB
# info: https://kb.askmonty.org/en/about-the-mariadb-java-client/
#sqldump.driverclass=org.mariadb.jdbc.Driver
#sqldump.dburl=jdbc:mysql://<host>:<port>/<database>
#sqldump.user=
#sqldump.password=

# Neo4j
# info: https://github.com/neo4j-contrib/neo4j-jdbc
#sqldump.driverclass=org.neo4j.jdbc.Driver
#sqldump.dburl=jdbc:neo4j://<host>:<port>/
#sqldump.dburl=jdbc:neo4j://localhost:7474/
#sqldump.user=
#sqldump.password=

# SQLite
#sqldump.driverclass=org.sqlite.JDBC
#sqldump.dburl=jdbc:sqlite:</path/to/sqlite.db>

# H2
# info: http://www.h2database.com/html/download.html
#sqldump.driverclass=org.h2.Driver
#sqldump.dburl=jdbc:h2:~/test

# Monet DB
# info: http://dev.monetdb.org/downloads/Java/Latest/
#sqldump.driverclass=nl.cwi.monetdb.jdbc.MonetDriver
#sqldump.dburl=jdbc:monetdb://localhost/demo
#sqldump.user=monetdb
#sqldump.password=monetdb

# Drizzle
# info: http://www.drizzle.org/
#sqldump.driverclass=org.drizzle.jdbc.DrizzleDriver
#sqldump.dburl=jdbc:drizzle://<user>@<host>:<port>/<database>
#sqldump.dburl=jdbc:mysql:thin://<user>@<host>:<port>/<database>
#sqldump.user=root
#sqldump.password=

# Firebird
# info: http://jaybirdwiki.firebirdsql.org/jaybird/doku.php?id=config:driver_config
#sqldump.driverclass=org.firebirdsql.jdbc.FBDriver
#sqldump.dburl=jdbc:firebirdsql://host[:port]/<database>
#sqldump.dburl=jdbc:firebirdsql://localhost:3050/examples/empbuild/employee.fdb
#sqldump.user=sysdba
#sqldump.password=masterke

# Virtuoso
# info: http://docs.openlinksw.com/virtuoso/VirtuosoDriverJDBC.html
#sqldump.driverclass=virtuoso.jdbc4.Driver
#sqldump.dburl=jdbc:virtuoso://hostname:port/UID=dba/PWD=dba
#sqldump.dburl=jdbc:virtuoso://localhost
#sqldump.user=dba
#sqldump.password=dba

# SQL Server - jTDS
# info: http://jtds.sourceforge.net/faq.html
#sqldump.driverclass=net.sourceforge.jtds.jdbc.Driver
#sqldump.dburl=jdbc:jtds:<server_type>://<server>[:<port>][/<database>][;<property>=<value>[;...]]
#sqldump.dburl=jdbc:jtds:sqlserver://localhost;domain=XYZ
#sqldump.user=
#sqldump.password=

# SQL Pivot ('connection decorator' pivot driver)
#sqldump.driverclass=tbrugz.sqldump.pivot.SQLPivotDriver
#sqldump.dburl=jdbc:sqlpivot:<other-jdbc-url>
#sqldump.dburl=jdbc:sqlpivot:h2:mem:
#sqldump.user=
#sqldump.password=


# Connection from datasource/jndi/pool
#sqldump.datasource=jdbc/LocalTestDB
# initial context lookup (default is 'java:/comp/env')
#sqldump.datasource.contextlookup=java:/
